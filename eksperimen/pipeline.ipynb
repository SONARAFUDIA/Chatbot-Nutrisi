{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5670a488-7d78-4236-809a-17c42b29ecff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre-Pipeline: Import and combine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e01e31e-4096-4278-afd3-5f0c2b5aa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('original_dataset/hellosehat_dataset_10.csv', sep=';', encoding='utf-8')\n",
    "df2 = pd.read_csv('original_dataset/alodokter_dataset_10.csv', sep=';', encoding='utf-8')\n",
    "df3 = pd.read_csv('original_dataset/doktersehat_gizi_final_10.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45fe201-1e73-40a7-a122-178c1278ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dokumen setelah concat: 2391\n",
      "Index range: 0 - 2390\n",
      "Kolom: ['URL', 'Judul', 'Konten']\n"
     ]
    }
   ],
   "source": [
    "df_combine = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(f\"Total dokumen setelah concat: {len(df_combine)}\")\n",
    "print(f\"Index range: {df_combine.index.min()} - {df_combine.index.max()}\")\n",
    "print(f\"Kolom: {df_combine.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0983142-15de-423f-bfe9-856d8eec8d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Judul</th>\n",
       "      <th>Konten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Kenali 9 Penyebab Perut Buncit dan Cara Mengat...</td>\n",
       "      <td>Perut buncit memang mampu memengaruhi penampil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hellosehat.com/nutrisi/tips-makan-seha...</td>\n",
       "      <td>8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue</td>\n",
       "      <td>Bagi Anda yang gemar bikin kue, oven gas menja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hellosehat.com/nutrisi/fakta-gizi/mere...</td>\n",
       "      <td>10 Merek Oatmeal yang Bergizi dan Cocok untuk ...</td>\n",
       "      <td>Butuh menu sarapan yang cepat? Berbagai merek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hellosehat.com/nutrisi/resep-sehat/jus...</td>\n",
       "      <td>4 Resep Jus untuk Bantu Meningkatkan Sistem Im...</td>\n",
       "      <td>Setiap harinya, sistem imunitas pada tubuh bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Apakah Sering Buang Air Bisa Menurunkan Berat ...</td>\n",
       "      <td>Setelah diolah, dicerna, dan diambil semua giz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "1  https://hellosehat.com/nutrisi/tips-makan-seha...   \n",
       "2  https://hellosehat.com/nutrisi/fakta-gizi/mere...   \n",
       "3  https://hellosehat.com/nutrisi/resep-sehat/jus...   \n",
       "4  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "\n",
       "                                               Judul  \\\n",
       "0  Kenali 9 Penyebab Perut Buncit dan Cara Mengat...   \n",
       "1    8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue   \n",
       "2  10 Merek Oatmeal yang Bergizi dan Cocok untuk ...   \n",
       "3  4 Resep Jus untuk Bantu Meningkatkan Sistem Im...   \n",
       "4  Apakah Sering Buang Air Bisa Menurunkan Berat ...   \n",
       "\n",
       "                                              Konten  \n",
       "0  Perut buncit memang mampu memengaruhi penampil...  \n",
       "1  Bagi Anda yang gemar bikin kue, oven gas menja...  \n",
       "2  Butuh menu sarapan yang cepat? Berbagai merek ...  \n",
       "3  Setiap harinya, sistem imunitas pada tubuh bek...  \n",
       "4  Setelah diolah, dicerna, dan diambil semua giz...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d15e7e-91e5-4582-b503-b4e902ad18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX that thang!\n",
    "# df_combine.to_csv('output_dataset/combined_nutrition_dataset_2.csv', index=False, encoding='utf-8', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240d6b6f-ee09-46f7-8ef3-12d7e0e96db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Judul</th>\n",
       "      <th>Konten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Kenali 9 Penyebab Perut Buncit dan Cara Mengat...</td>\n",
       "      <td>Perut buncit memang mampu memengaruhi penampil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hellosehat.com/nutrisi/tips-makan-seha...</td>\n",
       "      <td>8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue</td>\n",
       "      <td>Bagi Anda yang gemar bikin kue, oven gas menja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hellosehat.com/nutrisi/fakta-gizi/mere...</td>\n",
       "      <td>10 Merek Oatmeal yang Bergizi dan Cocok untuk ...</td>\n",
       "      <td>Butuh menu sarapan yang cepat? Berbagai merek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hellosehat.com/nutrisi/resep-sehat/jus...</td>\n",
       "      <td>4 Resep Jus untuk Bantu Meningkatkan Sistem Im...</td>\n",
       "      <td>Setiap harinya, sistem imunitas pada tubuh bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Apakah Sering Buang Air Bisa Menurunkan Berat ...</td>\n",
       "      <td>Setelah diolah, dicerna, dan diambil semua giz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "1  https://hellosehat.com/nutrisi/tips-makan-seha...   \n",
       "2  https://hellosehat.com/nutrisi/fakta-gizi/mere...   \n",
       "3  https://hellosehat.com/nutrisi/resep-sehat/jus...   \n",
       "4  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "\n",
       "                                               Judul  \\\n",
       "0  Kenali 9 Penyebab Perut Buncit dan Cara Mengat...   \n",
       "1    8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue   \n",
       "2  10 Merek Oatmeal yang Bergizi dan Cocok untuk ...   \n",
       "3  4 Resep Jus untuk Bantu Meningkatkan Sistem Im...   \n",
       "4  Apakah Sering Buang Air Bisa Menurunkan Berat ...   \n",
       "\n",
       "                                              Konten  \n",
       "0  Perut buncit memang mampu memengaruhi penampil...  \n",
       "1  Bagi Anda yang gemar bikin kue, oven gas menja...  \n",
       "2  Butuh menu sarapan yang cepat? Berbagai merek ...  \n",
       "3  Setiap harinya, sistem imunitas pada tubuh bek...  \n",
       "4  Setelah diolah, dicerna, dan diambil semua giz...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('output_dataset/combined_nutrition_dataset.csv', sep=';', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad3623e6-a206-41f4-855c-2f3735e0fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL       https://doktersehat.com/gaya-hidup/gizi-dan-nu...\n",
      "Judul        Sumber, Manfaat, dan Dampak Kekurangan Omega-3\n",
      "Konten     Kita selalu menganggap kalau omega-3 adalah n...\n",
      "Name: 2150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "row = df.loc[2150]\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef974a-662b-448a-aea8-f8f33c409842",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre-Pipeline: Intent Tagging (Health Goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddde1b5b-86f7-4c8d-9797-fb69ff3c761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1008306-2c1f-46f1-a3b8-1140ec3bd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. KONFIGURASI INPUT & OUTPUT ---\n",
    "INPUT_FILE = 'output_dataset/combined_nutrition_dataset.csv'\n",
    "OUTPUT_FILE = 'output_dataset/tagged_combined_nutrition_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7d2538-55a2-4ad3-9190-b7ec15f0b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. KAMUS KEYWORD INTENT ---\n",
    "INTENT_KEYWORDS = {\n",
    "    'diabetes': ['diabetes', 'gula darah', 'kencing manis', 'insulin', 'glukosa', 'hiperglikemia'],\n",
    "    'anemia': ['anemia', 'kurang darah', 'zat besi', 'hemoglobin', 'pucat', 'lelah'],\n",
    "    'kesehatan_ibu': ['hamil', 'menyusui', 'bumil', 'busui', 'asi', 'janin', 'kandungan', 'kehamilan'],\n",
    "    'kesehatan_anak': ['anak', 'bayi', 'balita', 'si kecil', 'tumbuh kembang', 'imunisasi'],\n",
    "    'berat_badan': ['berat badan', 'diet', 'kurus', 'gemuk', 'langsing', 'turun berat', 'buncit', 'lemak', 'kalori', 'obesitas'],\n",
    "    'pembentukan_tubuh': ['otot', 'gym', 'fitness', 'binaraga', 'sixpack', 'latihan beban', 'workout', 'massa otot'],\n",
    "    'kesehatan_pencernaan': ['pencernaan', 'usus', 'lambung', 'maag', 'gerd', 'sembelit', 'diare', 'serat'],\n",
    "    'resep_sehat': ['resep', 'cara membuat', 'bahan-bahan', 'menu masakan', 'cara masak', 'hidangan'],\n",
    "    'diet_khusus': ['keto', 'vegan', 'vegetarian', 'gluten free', 'rendah garam', 'dash diet', 'intermittent'],\n",
    "    'pencegahan': ['mencegah', 'risiko', 'hindari', 'bahaya', 'waspada', 'gejala', 'tanda-tanda'],\n",
    "    'fakta_gizi': ['kandungan gizi', 'nutrisi', 'protein', 'karbohidrat', 'vitamin', 'mineral', 'takaran saji'],\n",
    "    'makanan_sehat': ['buah', 'sayur', 'organik', 'superfood', 'makanan sehat', 'bijian', 'kacang'],\n",
    "    # Fallback\n",
    "    'Kesehatan_umum': ['manfaat', 'khasiat', 'sehat', 'bugar', 'stamina', 'daya tahan', 'imun', 'kesehatan']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c96a7416-7d41-4040-aa40-11eebc72edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_3_intents(text):\n",
    "    \"\"\"\n",
    "    Menghitung frekuensi keyword dan mengembalikan maksimal 3 intent teratas.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for intent, keywords in INTENT_KEYWORDS.items():\n",
    "        count = 0\n",
    "        for kw in keywords:\n",
    "            count += text.count(kw)\n",
    "        if count > 0:\n",
    "            scores[intent] = count\n",
    "    \n",
    "    # Jika tidak ada match, return default\n",
    "    if not scores:\n",
    "        return \"Kesehatan_umum\"\n",
    "    \n",
    "    # Urutkan score tertinggi -> terendah\n",
    "    sorted_intents = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Ambil 3 teratas\n",
    "    top_3 = [item[0] for item in sorted_intents[:3]]\n",
    "    return \", \".join(top_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "288ce2ad-f4fa-4ee9-af29-702e23b8ef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Membaca file input: output_dataset/combined_nutrition_dataset.csv ...\n",
      "üìä Total Data: 2425 baris.\n",
      "üîç Melakukan Auto-Tagging Intent...\n",
      "üíæ Menyimpan ke format final (Comma Separated + Quoted)...\n",
      "‚úÖ SUKSES! File tersimpan: output_dataset/tagged_combined_nutrition_dataset.csv\n",
      "   Format: \"URL\",\"Title\",\"Content\",\"Intent\" (Comma Separated)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. EKSEKUSI UTAMA ---\n",
    "def process_tagging():\n",
    "    print(f\"üìÇ Membaca file input: {INPUT_FILE} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Membaca file dengan separator ';'\n",
    "        # Kita asumsikan baris pertama adalah header [URL;Judul;Konten]\n",
    "        df = pd.read_csv(INPUT_FILE, sep=';', on_bad_lines='skip')\n",
    "        \n",
    "        # Validasi kolom dasar (Sesuaikan nama kolom jika di file Anda berbeda)\n",
    "        # Kita rename standar agar mudah diproses\n",
    "        # Asumsi urutan kolom: 1. URL, 2. Judul, 3. Konten\n",
    "        if len(df.columns) >= 3:\n",
    "            df.columns = ['url', 'title', 'content'] + list(df.columns[3:])\n",
    "        else:\n",
    "            print(\"‚ùå Error: File input harus memiliki minimal 3 kolom (URL;Judul;Konten)\")\n",
    "            return\n",
    "\n",
    "        print(f\"üìä Total Data: {len(df)} baris.\")\n",
    "        \n",
    "        # Bersihkan data (isi yang kosong dengan string kosong)\n",
    "        df['title'] = df['title'].fillna('').astype(str)\n",
    "        df['content'] = df['content'].fillna('').astype(str)\n",
    "        \n",
    "        print(\"üîç Melakukan Auto-Tagging Intent...\")\n",
    "        # Gabungkan Judul + Konten untuk pencarian keyword yang lebih akurat\n",
    "        df['full_text_scan'] = df['title'] + \" \" + df['content']\n",
    "        df['intent'] = df['full_text_scan'].apply(get_top_3_intents)\n",
    "        \n",
    "        # Hapus kolom bantuan\n",
    "        df.drop(columns=['full_text_scan'], inplace=True)\n",
    "        \n",
    "        print(f\"üíæ Menyimpan ke format final (Comma Separated + Quoted)...\")\n",
    "        \n",
    "        # --- TEKNIK PENYIMPANAN PENTING ---\n",
    "        # quoting=csv.QUOTE_ALL : Memaksa SEMUA kolom dibungkus tanda kutip \"...\"\n",
    "        # Ini menjamin koma di dalam teks TIDAK akan dianggap sebagai pemisah kolom baru.\n",
    "        df.to_csv(\n",
    "            OUTPUT_FILE, \n",
    "            sep=',',              # Separator Koma\n",
    "            quotechar='\"',        # Pembungkus Tanda Kutip Ganda\n",
    "            quoting=csv.QUOTE_ALL, # MODE AMAN: Bungkus semua data dengan kutip\n",
    "            index=False,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ SUKSES! File tersimpan: {OUTPUT_FILE}\")\n",
    "        print(\"   Format: \\\"URL\\\",\\\"Title\\\",\\\"Content\\\",\\\"Intent\\\" (Comma Separated)\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {INPUT_FILE} tidak ditemukan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Terjadi kesalahan: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2710d3-660b-44e8-a980-bb0db5dbbe54",
   "metadata": {},
   "source": [
    "# Tahap 1: Preprocessing and chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51677a25-e972-4303-8bc2-423413df53a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1a411-75b5-49bd-883f-ec83ea4b04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('output_dataset/tagged_combined_nutrition_dataset.csv')\n",
    "print(f\"Total dokumen: {len(df)}\")\n",
    "print(f\"Kolom dataset: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140e8fc-504b-49f8-894d-3829b083b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IndoBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Normalisasi teks\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s.,!?%-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Fungsi chunking dengan sliding window\n",
    "def chunk_text_with_overlap(text, max_tokens=384, overlap=50):\n",
    "    \"\"\"\n",
    "    Chunking dengan overlap sliding window\n",
    "    max_tokens: 512 - 128 (buffer untuk question) = 384\n",
    "    overlap: 50 tokens\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a478aa-4ed5-4d65-b52a-882228adaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proses semua dokumen\n",
    "processed_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    title = row.get('title', '')\n",
    "    content = row.get('content', '')\n",
    "    url = row.get('url', '')\n",
    "    intent = row.get('intent', '')\n",
    "    \n",
    "    full_text = f\"{title}. {content}\".strip()\n",
    "    cleaned_text = preprocess_text(full_text)\n",
    "    \n",
    "    if not cleaned_text:\n",
    "        continue\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(cleaned_text)\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        processed_data.append({\n",
    "            'doc_id': idx,\n",
    "            'chunk_id': chunk_idx,\n",
    "            'title': title,\n",
    "            'text': chunk,\n",
    "            'url': url,\n",
    "            'intent': intent,\n",
    "            'token_count': len(tokenizer.tokenize(chunk))\n",
    "        })\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} documents\")\n",
    "\n",
    "# Simpan hasil\n",
    "df_processed = pd.DataFrame(processed_data)\n",
    "df_processed.to_csv('output_dataset/processed_chunks.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(df_processed)}\")\n",
    "print(f\"Rata-rata tokens per chunk: {df_processed['token_count'].mean():.2f}\")\n",
    "print(f\"Max tokens: {df_processed['token_count'].max()}\")\n",
    "print(f\"Min tokens: {df_processed['token_count'].min()}\")\n",
    "\n",
    "# Simpan corpus untuk MLM\n",
    "all_texts = df_processed['text'].tolist()\n",
    "with open('corpus_for_mlm.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nFile tersimpan:\")\n",
    "print(\"- processed_chunks.csv\")\n",
    "print(\"- corpus_for_mlm.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6a194-3675-4c7f-b714-77518f400d94",
   "metadata": {},
   "source": [
    "# Tahap 2: Domain ~~EXPANSION~~ Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd5d79-d6b7-416b-8c13-675e900ef7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForMaskedLM,\n",
    "    DataCollatorForWholeWordMask,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bf4a4b-67e9-4645-9e9e-eb4616503137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU Ditemukan: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   VRAM Total: 6.44 GB\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP GPU & CLEAR MEMORY ---\n",
    "# Cek apakah CUDA (GPU) tersedia\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"‚úÖ GPU Ditemukan: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Bersihkan cache memori sebelumnya (Penting jika menggunakan Notebook)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"GPU tidak ditemukan! Menggunakan CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e1d0ce-a40d-4eb2-ba1b-667baa1a87df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts: 7661\n",
      "Data Training: 6894\n",
      "Data Validasi: 767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. LOAD DATA ---\n",
    "# Load corpus\n",
    "try:\n",
    "    with open('corpus_for_mlm.json', 'r', encoding='utf-8') as f:\n",
    "        all_texts = json.load(f)\n",
    "    print(f\"Total texts: {len(all_texts)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File corpus tidak ditemukan, menggunakan dummy data.\")\n",
    "    all_texts = [\"1\", \"2\"]\n",
    "\n",
    "# SPLIT DATA (90% Train, 10% Validation)\n",
    "# Kita butuh ini agar model bisa diuji (Val Loss muncul)\n",
    "split_idx = int(0.9 * len(all_texts))\n",
    "train_texts = all_texts[:split_idx]\n",
    "val_texts = all_texts[split_idx:]\n",
    "\n",
    "print(f\"Data Training: {len(train_texts)}\")\n",
    "print(f\"Data Validasi: {len(val_texts)}\")\n",
    "\n",
    "# Load model dan tokenizer\n",
    "model_name = 'indobenchmark/indobert-base-p2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d03145-5895-4d52-a9c2-63fba39f1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mempersiapkan Training Set...\n",
      "Preprocessing texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274fa7e28e6d44b984c4c94daa78d773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/6894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mempersiapkan Validation Set...\n",
      "Preprocessing texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26feedc38a243c6939aacb9e9eb96ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\data\\data_collator.py:1642: FutureWarning: DataCollatorForWholeWordMask is deprecated and will be removed in a future version, you can now use DataCollatorForLanguageModeling with whole_word_mask=True instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 3. PINDAHKAN MODEL KE GPU ---\n",
    "model.to(device)\n",
    "\n",
    "# Custom Dataset untuk MLM\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Preprocess dengan progress bar\n",
    "        print(\"Preprocessing texts...\")\n",
    "        self.encodings = []\n",
    "        for text in tqdm(texts, desc=\"Tokenizing\"):\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.encodings.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze()\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings[idx]\n",
    "\n",
    "# Custom Data Collator\n",
    "class CustomWWMDataCollator(DataCollatorForWholeWordMask):\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15):\n",
    "        super().__init__(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=True,\n",
    "            mlm_probability=mlm_probability\n",
    "        )\n",
    "    \n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask=None):\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        \n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "                for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "        \n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        \n",
    "        for i, input_ids in enumerate(inputs):\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            for j, token in enumerate(tokens):\n",
    "                if re.search(r'\\d', token) or token in ['mg', 'gram', 'kg', 'ml', 'kkal', 'kalori', '%']:\n",
    "                    probability_matrix[i, j] = 0.0\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "        return inputs, labels\n",
    "\n",
    "# Buat 2 Dataset Terpisah\n",
    "print(\"\\nMempersiapkan Training Set...\")\n",
    "train_dataset = MLMDataset(train_texts, tokenizer)\n",
    "\n",
    "print(\"Mempersiapkan Validation Set...\")\n",
    "val_dataset = MLMDataset(val_texts, tokenizer)\n",
    "\n",
    "# Data collator\n",
    "data_collator = CustomWWMDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f718a0-db49-4c58-a659-3812babd9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mulai Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2586' max='2586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2586/2586 46:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.563700</td>\n",
       "      <td>4.428384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.044200</td>\n",
       "      <td>3.764034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.597000</td>\n",
       "      <td>3.424848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.361500</td>\n",
       "      <td>3.217364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.164900</td>\n",
       "      <td>3.068305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.013400</td>\n",
       "      <td>2.934664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.841500</td>\n",
       "      <td>2.784796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.731400</td>\n",
       "      <td>2.662641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.627900</td>\n",
       "      <td>2.553934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.576000</td>\n",
       "      <td>2.442628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.479600</td>\n",
       "      <td>2.399022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.410200</td>\n",
       "      <td>2.328005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.376600</td>\n",
       "      <td>2.218874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.286200</td>\n",
       "      <td>2.147584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.237600</td>\n",
       "      <td>2.087961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.122400</td>\n",
       "      <td>2.026285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.082000</td>\n",
       "      <td>1.984129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.026600</td>\n",
       "      <td>1.909266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.987600</td>\n",
       "      <td>1.892458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.956700</td>\n",
       "      <td>1.861038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.945200</td>\n",
       "      <td>1.850937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.940900</td>\n",
       "      <td>1.826260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.896500</td>\n",
       "      <td>1.831095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.881300</td>\n",
       "      <td>1.827258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.837200</td>\n",
       "      <td>1.797632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan model...\n",
      "Selesai!\n"
     ]
    }
   ],
   "source": [
    "# --- 4. TRAINING ARGUMENTS ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./indobert-gizi-mlm',\n",
    "    overwrite_output_dir=True,\n",
    "      \n",
    "    # === HYPERPARAMETERS ===\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,              \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # === EVALUATION SETTINGS (BARU) ===\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    \n",
    "    # === LOGGING ===\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # === SYSTEM ===\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    no_cuda=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,       \n",
    "    disable_tqdm=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\nMulai Training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Simpan Model Akhir\n",
    "print(\"\\nMenyimpan model...\")\n",
    "model.save_pretrained(\"./indobert-gizi-mlm-final\")\n",
    "tokenizer.save_pretrained(\"./indobert-gizi-mlm-final\")\n",
    "print(\"Selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6cef1e-994c-47bf-b628-dfce7cd58aae",
   "metadata": {},
   "source": [
    "## Clear Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea5ee79-d37b-48d3-abfa-f113fed57604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc \n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c708a-5302-43ae-8d14-181084d2d112",
   "metadata": {},
   "source": [
    "# Tahap 3: Membuat Pasangan QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96b67f4-1dda-4440-bd2d-af51cd56cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from groq import Groq, RateLimitError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df678427-6259-4cdd-a5c4-b8b794704d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. KONFIGURASI ---\n",
    "API_KEY = \"apikey\" \n",
    "client = Groq(api_key=API_KEY)\n",
    "\n",
    "# Model\n",
    "PRIMARY_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# File Paths (Sesuaikan dengan notebook Anda)\n",
    "INPUT_FILE = 'output_dataset/dataset_filtered_processed_chunks.csv'\n",
    "OUTPUT_FILE = '3_qa_dataset_strict_gold.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286c92e8-318e-4202-ad56-f8347ef6d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. FUNGSI GENERATOR STRICT ---\n",
    "def generate_qa_strict(context):\n",
    "    prompt = f\"\"\"\n",
    "    You are a strict Data Annotator for a SQuAD dataset.\n",
    "    \n",
    "    TASK: Extract 3 QA pairs from the provided text.\n",
    "    \n",
    "    CRITICAL RULES:\n",
    "    1. The \"answer\" MUST be an EXACT SUBSTRING from the Context. Copy-paste ONLY. No paraphrasing.\n",
    "    2. Provide 2 answerable questions and 1 unanswerable question.\n",
    "    3. Language: Indonesian.\n",
    "    \n",
    "    CONTEXT:\n",
    "    \"{context}\"\n",
    "    \n",
    "    OUTPUT FORMAT (JSON ONLY):\n",
    "    [\n",
    "      {{ \"question\": \"Pertanyaan?\", \"answer\": \"exact substring\", \"is_impossible\": false }},\n",
    "      {{ \"question\": \"Pertanyaan sulit?\", \"answer\": \"\", \"is_impossible\": true }}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    max_retries = 5\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You output JSON only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                model=PRIMARY_MODEL,\n",
    "                temperature=0.0, # Wajib 0 agar tidak kreatif (konsisten)\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            # --- LOGIKA MENANGANI RATE LIMIT (429) ---\n",
    "            print(f\"‚ö†Ô∏è Rate Limit Hit! (Percobaan {retry_count + 1}/{max_retries})\")\n",
    "            \n",
    "            # Coba ambil informasi waktu tunggu dari header response\n",
    "            retry_after = 0\n",
    "            try:\n",
    "                headers = e.response.headers\n",
    "                \n",
    "                # Coba header standar 'retry-after'\n",
    "                if 'retry-after' in headers:\n",
    "                    retry_after = float(headers['retry-after'])\n",
    "                \n",
    "                # Coba header spesifik 'x-ratelimit-reset-tokens' (biasanya format \"12.34s\")\n",
    "                elif 'x-ratelimit-reset-tokens' in headers:\n",
    "                    reset_tokens = headers['x-ratelimit-reset-tokens']\n",
    "                    retry_after = float(reset_tokens.replace('s', ''))\n",
    "                \n",
    "                # Coba header spesifik 'x-ratelimit-reset-requests'\n",
    "                elif 'x-ratelimit-reset-requests' in headers:\n",
    "                    reset_reqs = headers['x-ratelimit-reset-requests']\n",
    "                    # Parse format \"12m30s\" atau \"12.34s\" jika perlu, tapi float simpel biasanya cukup untuk seconds\n",
    "                    if 'm' in reset_reqs: \n",
    "                        # Sederhana: jika ada menit, tunggu default 60 detik saja biar aman\n",
    "                        retry_after = 60 \n",
    "                    else:\n",
    "                        retry_after = float(reset_reqs.replace('s', ''))\n",
    "            \n",
    "            except Exception as header_err:\n",
    "                # Jika gagal parsing header, gunakan default\n",
    "                print(f\"   (Gagal baca header: {header_err})\")\n",
    "                retry_after = 20 # Default aman\n",
    "\n",
    "            # Pastikan retry_after minimal 1 detik & tambahkan buffer\n",
    "            wait_time = max(1, retry_after) + 2 \n",
    "            \n",
    "            print(f\"‚è≥ Tidur selama {wait_time:.2f} detik sebelum mencoba lagi...\")\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "            retry_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Error lain (misal koneksi putus, JSON malformed dari server, dll)\n",
    "            print(f\"‚ùå Error non-limit: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"‚ùå Gagal setelah max retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2f12a6-7cc1-4ccb-97a4-cbd8c70da030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_strict_pipeline():\n",
    "    print(f\"‚öôÔ∏è Menyiapkan Pipeline Strict Mode dengan Model: {PRIMARY_MODEL}\")\n",
    "    \n",
    "    # Baca Data Input\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_FILE)\n",
    "        # Ambil kolom text, pastikan string\n",
    "        texts = df['text'].fillna('').astype(str).unique()\n",
    "        total_awal = len(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal baca input: {e}\")\n",
    "        return\n",
    "\n",
    "    # Logika Resume (Cek data yg sudah ada)\n",
    "    processed_contexts = set()\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"üìÇ Membaca checkpoint dari {OUTPUT_FILE}...\")\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    processed_contexts.add(data['context'])\n",
    "                except: continue\n",
    "    \n",
    "    # Filter data yang belum diproses\n",
    "    texts_to_process = [t for t in texts if t not in processed_contexts]\n",
    "    print(f\"üìä Sisa data: {len(texts_to_process)} dari {total_awal} total chunk unik.\")\n",
    "    \n",
    "    if not texts_to_process:\n",
    "        print(\"üéâ Semua data sudah selesai!\")\n",
    "        return\n",
    "\n",
    "    valid_count = 0\n",
    "    dropped_count = 0\n",
    "    \n",
    "    # Loop Utama\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "        for text in tqdm(texts_to_process, desc=\"Generating QA\"):\n",
    "            if len(text) < 50: continue\n",
    "            \n",
    "            # Request ke LLM (Pakai fungsi SMART)\n",
    "            qa_list = generate_qa_strict(text)\n",
    "            \n",
    "            # Parsing response\n",
    "            if qa_list:\n",
    "                # Handle variasi struktur JSON dari LLM\n",
    "                if isinstance(qa_list, dict): \n",
    "                    qa_list = qa_list.get('qas', qa_list.get('qa_pairs', []))\n",
    "                \n",
    "                squad_qas = []\n",
    "                # Pastikan qa_list adalah list sebelum iterasi\n",
    "                if isinstance(qa_list, list):\n",
    "                    for item in qa_list:\n",
    "                        # Validasi Format\n",
    "                        if not isinstance(item, dict): continue\n",
    "                        \n",
    "                        question = item.get('question')\n",
    "                        answer_text = item.get('answer', '')\n",
    "                        is_impossible = item.get('is_impossible', False)\n",
    "                        \n",
    "                        if is_impossible:\n",
    "                            squad_qas.append({\n",
    "                                \"id\": str(uuid.uuid4()),\n",
    "                                \"question\": question,\n",
    "                                \"answers\": [],\n",
    "                                \"is_impossible\": True\n",
    "                            })\n",
    "                        else:\n",
    "                            # --- STRICT CHECK (Validasi Substring) ---\n",
    "                            if not answer_text: continue\n",
    "                            \n",
    "                            # Case insensitive search untuk fleksibilitas sedikit\n",
    "                            # Tapi kita simpan text ASLI dari konteks\n",
    "                            start_idx = text.find(answer_text)\n",
    "                            \n",
    "                            if start_idx != -1:\n",
    "                                # LOLOS VALIDASI\n",
    "                                squad_qas.append({\n",
    "                                    \"id\": str(uuid.uuid4()),\n",
    "                                    \"question\": question,\n",
    "                                    \"answers\": [{\n",
    "                                        \"text\": answer_text,\n",
    "                                        \"answer_start\": start_idx\n",
    "                                    }],\n",
    "                                    \"is_impossible\": False\n",
    "                                })\n",
    "                                valid_count += 1\n",
    "                            else:\n",
    "                                # GAGAL VALIDASI (Halusinasi LLM) -> BUANG\n",
    "                                dropped_count += 1\n",
    "                \n",
    "                # Simpan jika ada QA valid\n",
    "                if squad_qas:\n",
    "                    entry = {\"context\": text, \"qas\": squad_qas}\n",
    "                    f.write(json.dumps(entry) + \"\\n\")\n",
    "                    f.flush()\n",
    "            \n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üèÅ PROSES SELESAI\")\n",
    "    print(f\"‚úÖ QA Valid Tersimpan: {valid_count}\")\n",
    "    print(f\"üóëÔ∏è QA Halusinasi Dibuang: {dropped_count}\")\n",
    "    print(f\"üìÇ Output: {OUTPUT_FILE}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52954aa1-7e4b-4be4-80e0-fbf5ab129979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Menyiapkan Pipeline Strict Mode dengan Model: llama-3.3-70b-versatile\n",
      "üìÇ Membaca checkpoint dari 3_qa_dataset_strict_gold.jsonl...\n",
      "üìä Sisa data: 38 dari 752 total chunk unik.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [02:08<00:00,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèÅ PROSES SELESAI\n",
      "‚úÖ QA Valid Tersimpan: 56\n",
      "üóëÔ∏è QA Halusinasi Dibuang: 20\n",
      "üìÇ Output: 3_qa_dataset_strict_gold.jsonl\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Jalankan\n",
    "if __name__ == \"__main__\":\n",
    "    run_strict_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b7537-6d6c-43fe-a8ab-d03b51157d20",
   "metadata": {},
   "source": [
    "# Tahap 4: Fine-Tuning QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fcedfd-43ee-4ad6-9fa2-ff54375f07bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import gc\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertModel, BertConfig \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0f69b0-d6d6-4c93-920f-0336569de1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   VRAM: 6.44 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU tidak tersedia, menggunakan CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1059711a-d032-4875-8488-f04d4562fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraf: 752\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = []\n",
    "with open('3_qa_dataset_strict_gold.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        qa_pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Total paragraf: {len(qa_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7a7ed8-a5d4-45b1-a900-c91fdee80c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Loading BERT encoder dari Domain Adaptation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./indobert-gizi-mlm-final and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model QA berhasil diinisialisasi.\n",
      "üîç Cek QA Outputs Layer: Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model dari Domain Adaptation\n",
    "model_path = './indobert-gizi-mlm-final'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Load base BERT dari MLM model, lalu inisialisasi QA head baru\n",
    "from transformers import BertConfig, BertForQuestionAnswering\n",
    "\n",
    "print(\"\\nüîß Loading BERT encoder dari Domain Adaptation...\")\n",
    "\n",
    "# === PERBAIKAN DI SINI ===\n",
    "# Kita paksa num_labels=2 agar output layer QA hanya 2 (Start & End)\n",
    "config = BertConfig.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Gunakan .from_pretrained langsung dengan ignore_mismatched_sizes=True\n",
    "# Ini akan memuat body BERT yang sudah di-training (MLM), tapi mereset head QA menjadi benar\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "    model_path, \n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model QA berhasil diinisialisasi.\")\n",
    "print(f\"üîç Cek QA Outputs Layer: {model.qa_outputs}\") \n",
    "# Pastikan output print di atas tertulis out_features=2\n",
    "\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ae9ec8-e46a-439c-9cff-e3945bae36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs: 1911\n"
     ]
    }
   ],
   "source": [
    "# Parse data ke format training\n",
    "train_examples = []\n",
    "\n",
    "for para in qa_pairs:\n",
    "    context = para['context']\n",
    "    for qa in para['qas']:\n",
    "        example = {\n",
    "            'id': qa['id'],\n",
    "            'question': qa['question'],\n",
    "            'context': context,\n",
    "        }\n",
    "        \n",
    "        if qa['is_impossible']:\n",
    "            example['answers'] = {'answer_start': [], 'text': []}\n",
    "        else:\n",
    "            example['answers'] = {\n",
    "                'answer_start': [qa['answers'][0]['answer_start']],\n",
    "                'text': [qa['answers'][0]['text']]\n",
    "            }\n",
    "        \n",
    "        train_examples.append(example)\n",
    "\n",
    "print(f\"Total QA pairs: {len(train_examples)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95322459-3425-410f-bc7e-fc2f67a66250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1719\n",
      "Validation: 192\n"
     ]
    }
   ],
   "source": [
    "# Split train/validation (90/10)\n",
    "split_idx = int(0.9 * len(train_examples))\n",
    "train_data = train_examples[:split_idx]\n",
    "val_data = train_examples[split_idx:]\n",
    "\n",
    "print(f\"Training: {len(train_data)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16fe8ce9-da45-42c4-9511-4b9ec8b8e29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Tokenizing training data...\n",
      "üîÑ Tokenizing validation data...\n"
     ]
    }
   ],
   "source": [
    "# Tokenisasi dengan batch processing\n",
    "def tokenize_dataset(examples):\n",
    "    questions = [ex['question'] for ex in examples]\n",
    "    contexts = [ex['context'] for ex in examples]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation='only_second',\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
    "    offset_mapping = tokenized['offset_mapping']\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized['input_ids'][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[sample_index]['answers']\n",
    "        \n",
    "        # Unanswerable question\n",
    "        if len(answers['answer_start']) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "        \n",
    "        start_char = answers['answer_start'][0]\n",
    "        end_char = start_char + len(answers['text'][0])\n",
    "        \n",
    "        # Find token start position\n",
    "        token_start_index = 0\n",
    "        while token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        \n",
    "        # Find token end position\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while token_end_index >= 0 and sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        # Check if answer is in this chunk\n",
    "        if not (token_start_index < len(offsets) and \n",
    "                token_end_index < len(offsets) and\n",
    "                offsets[token_start_index][0] <= start_char and \n",
    "                offsets[token_end_index][1] >= end_char):\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "        \n",
    "        # Find exact token positions\n",
    "        while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "            token_start_index += 1\n",
    "        start_positions.append(token_start_index - 1)\n",
    "        \n",
    "        while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "            token_end_index -= 1\n",
    "        end_positions.append(token_end_index + 1)\n",
    "    \n",
    "    tokenized['start_positions'] = start_positions\n",
    "    tokenized['end_positions'] = end_positions\n",
    "    tokenized.pop('offset_mapping')\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"\\nüîÑ Tokenizing training data...\")\n",
    "tokenized_train = tokenize_dataset(train_data)\n",
    "\n",
    "print(\"üîÑ Tokenizing validation data...\")\n",
    "tokenized_val = tokenize_dataset(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e18169e-5ab8-4dea-a436-719e84db873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1719\n",
      "Val dataset size: 192\n"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][idx]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][idx])\n",
    "        }\n",
    "\n",
    "train_dataset = QADataset(tokenized_train)\n",
    "val_dataset = QADataset(tokenized_val)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0b6f17-cce5-415a-ab58-76a82f2f2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./indobert-gizi-qa',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "    disable_tqdm=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dc8cc6-c86d-41a5-80dd-bbb3fbfe3911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Memulai Fine-Tuning QA...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [645/645 07:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.953500</td>\n",
       "      <td>2.694164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.577300</td>\n",
       "      <td>2.419097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.803400</td>\n",
       "      <td>2.265416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.559100</td>\n",
       "      <td>2.194084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.059200</td>\n",
       "      <td>2.297399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.013200</td>\n",
       "      <td>2.223674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Menyimpan model...\n",
      "\n",
      "==================================================\n",
      "Fine-Tuning QA selesai!\n",
      "Model tersimpan di: ./indobert-gizi-qa-final\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Memulai Fine-Tuning QA...\")\n",
    "print(\"=\"*50)\n",
    "trainer.train()\n",
    "\n",
    "# Simpan model\n",
    "print(\"\\nüíæ Menyimpan model...\")\n",
    "model.save_pretrained('./indobert-gizi-qa-final')\n",
    "tokenizer.save_pretrained('./indobert-gizi-qa-final')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-Tuning QA selesai!\")\n",
    "print(\"Model tersimpan di: ./indobert-gizi-qa-final\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80a273-959f-48f5-9a43-8aa7378ed5a7",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5defa7ee-3e14-4d8f-9bae-201d962920e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ü§ñ HASIL TES MODEL QA\n",
      "==================================================\n",
      "Tanya : Apa itu diabetes melitus?\n",
      "Jawab : kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin\n",
      "Score : 0.1257\n",
      "------------------------------\n",
      "Tanya : Apa penyebab utama diabetes?\n",
      "Jawab : kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin\n",
      "Score : 0.1672\n",
      "------------------------------\n",
      "Tanya : Bagaimana cara mencegahnya diabetes melitus?\n",
      "Jawab : kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin\n",
      "Score : 0.0686\n",
      "------------------------------\n",
      "Tanya : Apa gejala Diabetes melitus?\n",
      "Jawab : kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin\n",
      "Score : 0.1180\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, pipeline\n",
    "\n",
    "# 1. Load Model Final\n",
    "model_path = \"./indobert-gizi-qa-final\" # Pastikan path ini sesuai output training\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# 2. Buat Pipeline QA\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 # Gunakan GPU (0) atau CPU (-1)\n",
    ")\n",
    "\n",
    "# 3. Tes Manual\n",
    "context_text = \"\"\"\n",
    "Diabetes melitus adalah penyakit kronis yang ditandai dengan kadar gula darah yang tinggi. \n",
    "Penyebab utamanya adalah kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin.\n",
    "Gejala umum meliputi sering haus, sering buang air kecil, dan penurunan berat badan drastis.\n",
    "Pencegahan dapat dilakukan dengan menjaga pola makan sehat dan rutin berolahraga.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Apa itu diabetes melitus?\",\n",
    "    \"Apa penyebab utama diabetes?\",\n",
    "    \"Bagaimana cara mencegahnya diabetes melitus?\",\n",
    "    \"Apa gejala Diabetes melitus?\"\n",
    "]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ü§ñ HASIL TES MODEL QA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for q in questions:\n",
    "    result = qa_pipeline(question=q, context=context_text)\n",
    "    print(f\"Tanya : {q}\")\n",
    "    print(f\"Jawab : {result['answer']}\")\n",
    "    print(f\"Score : {result['score']:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cf724-c243-4d16-be39-696e0128bb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce89d0e-45a4-45e3-a0d5-c376c9aa1e4b",
   "metadata": {},
   "source": [
    "# Tahap 5: Search FUnction (_or Search Engine, whaatever you called it_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dedd21d-d351-44b6-a19f-93aabaa10f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55abc1f-5d2e-4d18-ac9c-7beed708f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n",
      "Total chunks: 7661\n"
     ]
    }
   ],
   "source": [
    "# Load processed chunks\n",
    "print(\"üìÇ Loading dataset...\")\n",
    "df = pd.read_csv('output_dataset/processed_chunks.csv')\n",
    "print(f\"Total chunks: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0653ab-6f3b-4f89-8e77-04ffeabb47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading QA Model...\n",
      " QA Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load QA Model (Reader)\n",
    "print(\"\\n Loading QA Model...\")\n",
    "qa_model_path = './indobert-gizi-qa-final'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(qa_model_path)\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(qa_model_path)\n",
    "qa_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "qa_model.eval()\n",
    "print(\" QA Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef34c44b-f6ca-48a1-a8fa-9fd22c825d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d10b293d58a44da940856f959aefabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--LazarusNLP--all-indobert-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde71516862843e6840e0f8d65f24a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7378de47151e429db0743e8a5a4d3a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d16491f0ef40d29f8b4cd6c74a495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b8de0550a6448683b280fa78079312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e51e69b5d9746b588ec5c2ee152ec25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb26929fa5648a5adda04e94c8fd3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebf2ad876be475c95f90bb5b6d516e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca2a696f39d469f9acf34a977a49831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322f0de2f2924da4a6b721d2d6ac58fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045dd62211b6467ba35aa4b554459d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# RETRIEVER: Hybrid Search (Dense + BM25)\n",
    "# ============================================\n",
    "\n",
    "# Gunakan SentenceTransformer untuk embedding\n",
    "embedding_model = SentenceTransformer('LazarusNLP/all-indobert-base-v2')\n",
    "\n",
    "def custom_embedding_function(texts):\n",
    "    return embedding_model.encode(texts, show_progress_bar=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe97c643-6a77-4378-9e69-115300eb715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Setting up Retriever...\n",
      "  ‚Üí Initializing ChromaDB from: ./chroma_db_store\n"
     ]
    }
   ],
   "source": [
    "# Buat collection\n",
    "CHROMA_DATA_PATH = \"./chroma_db_store\" # Folder tempat data akan disimpan selamanya\n",
    "\n",
    "print(\"\\nüîç Setting up Retriever...\")\n",
    "\n",
    "# 1. Gunakan PERSISTENT Client (Bukan Client biasa)\n",
    "print(f\"  ‚Üí Initializing ChromaDB from: {CHROMA_DATA_PATH}\")\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "# Setup Embedding Function\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    ")\n",
    "\n",
    "# 2. Get or Create Collection\n",
    "# Fungsi ini akan meload collection jika ada, atau membuat baru jika belum ada\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"gizi_knowledge\",\n",
    "    embedding_function=embedding_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07076fab-1153-428b-8eea-0f61a252cad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Database ditemukan berisi 7661 dokumen.\n",
      "  ‚è© Skip indexing. Langsung siap dipakai.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOGIKA PENGISIAN DATA CERDAS\n",
    "# ============================================\n",
    "\n",
    "# Cek apakah database sudah berisi data?\n",
    "if collection.count() == 0:\n",
    "    print(\"  ‚ö†Ô∏è Database kosong. Memulai proses Indexing Dokumen (Hanya sekali)...\")\n",
    "    \n",
    "    # --- PROSES ADD DOCUMENT (Sama seperti kode lama Anda) ---\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        collection.add(\n",
    "            documents=batch['text'].tolist(),\n",
    "            ids=[f\"chunk_{j}\" for j in range(i, min(i+batch_size, len(df)))],\n",
    "            metadatas=[\n",
    "                {\n",
    "                    'title': str(row['title']),\n",
    "                    'url': str(row['url']),\n",
    "                    'intent': str(row['intent']),\n",
    "                    'doc_id': int(row['doc_id']),\n",
    "                    'chunk_id': int(row['chunk_id'])\n",
    "                }\n",
    "                for _, row in batch.iterrows()\n",
    "            ]\n",
    "        )\n",
    "        if (i + batch_size) % 500 == 0:\n",
    "            print(f\"    Processed {min(i+batch_size, len(df))}/{len(df)} chunks\")\n",
    "            \n",
    "    print(\"  ‚úÖ Indexing Selesai & Tersimpan Otomatis!\")\n",
    "\n",
    "else:\n",
    "    print(f\"  ‚úÖ Database ditemukan berisi {collection.count()} dokumen.\")\n",
    "    print(\"  ‚è© Skip indexing. Langsung siap dipakai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92af591d-34ed-400a-a91e-6e092e2ce575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí Initializing BM25...\n",
      "‚úÖ BM25 ready\n"
     ]
    }
   ],
   "source": [
    "# 2. BM25 (Sparse Retrieval)\n",
    "print(\"  ‚Üí Initializing BM25...\")\n",
    "tokenized_corpus = [doc.lower().split() for doc in df['text'].tolist()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(\"‚úÖ BM25 ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c699db97-57e1-42e7-9c41-474191b1016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENSEMBLE RETRIEVER\n",
    "# ============================================\n",
    "\n",
    "def hybrid_retrieve(query, top_k=5, dense_weight=0.6, bm25_weight=0.4):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval: Dense (ChromaDB) + Sparse (BM25)\n",
    "    \"\"\"\n",
    "    # Dense retrieval\n",
    "    dense_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k * 2\n",
    "    )\n",
    "    \n",
    "    dense_docs = []\n",
    "    for i, doc_id in enumerate(dense_results['ids'][0]):\n",
    "        idx = int(doc_id.split('_')[1])\n",
    "        dense_docs.append({\n",
    "            'idx': idx,\n",
    "            'text': dense_results['documents'][0][i],\n",
    "            'metadata': dense_results['metadatas'][0][i],\n",
    "            'distance': dense_results['distances'][0][i],\n",
    "            'dense_score': 1 / (1 + dense_results['distances'][0][i])  # Convert distance to score\n",
    "        })\n",
    "    \n",
    "    # BM25 retrieval\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[::-1][:top_k * 2]\n",
    "    \n",
    "    bm25_docs = []\n",
    "    for idx in top_bm25_indices:\n",
    "        bm25_docs.append({\n",
    "            'idx': int(idx),\n",
    "            'bm25_score': float(bm25_scores[idx])\n",
    "        })\n",
    "    \n",
    "    # Combine scores\n",
    "    combined = {}\n",
    "    for doc in dense_docs:\n",
    "        idx = doc['idx']\n",
    "        combined[idx] = {\n",
    "            'text': doc['text'],\n",
    "            'metadata': doc['metadata'],\n",
    "            'score': dense_weight * doc['dense_score']\n",
    "        }\n",
    "    \n",
    "    for doc in bm25_docs:\n",
    "        idx = doc['idx']\n",
    "        if idx in combined:\n",
    "            combined[idx]['score'] += bm25_weight * doc['bm25_score']\n",
    "        else:\n",
    "            combined[idx] = {\n",
    "                'text': df.iloc[idx]['text'],\n",
    "                'metadata': {\n",
    "                    'title': df.iloc[idx]['title'],\n",
    "                    'url': df.iloc[idx]['url'],\n",
    "                    'intent': df.iloc[idx]['intent']\n",
    "                },\n",
    "                'score': bm25_weight * doc['bm25_score']\n",
    "            }\n",
    "    \n",
    "    # Sort by combined score\n",
    "    sorted_results = sorted(combined.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            'text': item[1]['text'],\n",
    "            'metadata': item[1]['metadata'],\n",
    "            'score': item[1]['score']\n",
    "        }\n",
    "        for item in sorted_results[:top_k]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76fc958b-313f-4a92-b4f5-ff983bef1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, confidence_threshold=0.0): # Set 0.0 biar kita lihat SEMUA hasil\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    ).to(qa_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = qa_model(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits[0]\n",
    "    end_logits = outputs.end_logits[0]\n",
    "    \n",
    "    # Ambil posisi terbaik\n",
    "    start_idx = torch.argmax(start_logits).item()\n",
    "    end_idx = torch.argmax(end_logits).item()\n",
    "    \n",
    "    # Hitung probabilitas\n",
    "    start_probs = torch.softmax(start_logits, dim=0)\n",
    "    end_probs = torch.softmax(end_logits, dim=0)\n",
    "    confidence = (start_probs[start_idx] * end_probs[end_idx]).item()\n",
    "    \n",
    "    # --- BAGIAN DEBUGGING ---\n",
    "    print(f\"\\n   üïµÔ∏è [DEBUG] Q: {question}\")\n",
    "    print(f\"   üìç Start Index: {start_idx} | End Index: {end_idx}\")\n",
    "    print(f\"   üìä Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Cek apakah dia menunjuk [CLS] (Index 0)\n",
    "    if start_idx == 0:\n",
    "        print(\"   ‚ùå Model memprediksi: [CLS] (Unanswerable / Menyerah)\")\n",
    "        return None, confidence\n",
    "\n",
    "    # Cek apakah start > end (Mustahil)\n",
    "    if start_idx > end_idx:\n",
    "        print(\"   ‚ùå Model memprediksi: Posisi Terbalik (Start > End)\")\n",
    "        return None, confidence\n",
    "\n",
    "    # Extract jawaban\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    answer_tokens = tokens[start_idx:end_idx + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens).replace('##', '')\n",
    "    \n",
    "    print(f\"   ‚úÖ Calon Jawaban: {answer}\")\n",
    "    return answer, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56656a97-2b0a-4f2a-add2-194d96c7b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHATBOT PIPELINE (MODE BYPASS / SEARCH ENGINE)\n",
    "# ============================================\n",
    "\n",
    "def chatbot_pipeline(question, top_k=3):\n",
    "    print(f\"\\n Question: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Cari Dokumen (Retriever tetap bekerja!)\n",
    "    print(\" Retrieving relevant documents...\")\n",
    "    retrieved_docs = hybrid_retrieve(question, top_k=top_k)\n",
    "    \n",
    "    # Fallback jika tidak ada dokumen\n",
    "    if not retrieved_docs:\n",
    "        return {\n",
    "            'answer': 'Maaf, saya belum menemukan informasi yang relevan di database.',\n",
    "            'source': None,\n",
    "            'confidence': 0.0,\n",
    "            'intent': 'Unknown'\n",
    "        }\n",
    "        \n",
    "    print(f\"‚úÖ Found {len(retrieved_docs)} relevant documents\")\n",
    "    \n",
    "    # 2. BYPASS QA MODEL\n",
    "    # Alih-alih menyuruh model QA mikir, kita langsung ambil dokumen terbaik (Top 1)\n",
    "    best_doc = retrieved_docs[0] \n",
    "    \n",
    "    # Ambil snippet (misal 3 kalimat pertama atau 300 karakter)\n",
    "    full_text = best_doc['text']\n",
    "    # Potong biar gak kepanjangan\n",
    "    snippet = full_text[:350] + \"...\" if len(full_text) > 350 else full_text\n",
    "    \n",
    "    # Format jawaban ala Search Engine\n",
    "    response = (\n",
    "        f\"Berikut informasi yang saya temukan:\\n\\n\"\n",
    "        f\"\\\"{snippet}\\\"\\n\\n\"\n",
    "        f\"(Sumber: {best_doc['metadata']['title']})\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response,\n",
    "        'source': best_doc['metadata']['url'],\n",
    "        'confidence': best_doc['score'], # Pakai skor relevansi pencarian\n",
    "        'intent': best_doc['metadata']['intent']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "787ba9b5-ed38-4a95-b110-2209aa872e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing pipeline...\n",
      "\n",
      " Question: Apa penyebab perut buncit?\n",
      "============================================================\n",
      " Retrieving relevant documents...\n",
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      " Answer: Berikut informasi yang saya temukan:\n",
      "\n",
      "\"10 makanan penyebab perut buncit no . 7 sering dikonsumsi . makanan penyebab perut buncit bisa menjadi penyebab penumpukan lemak di perut . kondisi ini bisa meningkatkan berbagai risiko penyakit , namun masih bisa diatasi dengan asupan nutrisi yang tepat , meningkatkan aktivitas fisik , hingga mengurangi stres . lalu , apa saja makanan yang harus d...\"\n",
      "\n",
      "(Sumber: 10 Makanan Penyebab Perut Buncit (No. 7 Sering Dikonsumsi))\n",
      " Source: https://doktersehat.com/gaya-hidup/gizi-dan-nutrisi/makanan-penyebab-perut-buncit/\n",
      " Confidence: 399.97%\n",
      "  Intent: berat_badan, Kesehatan_umum, kesehatan_ibu\n",
      "\n",
      " Question: Bagaimana cara menurunkan berat badan?\n",
      "============================================================\n",
      " Retrieving relevant documents...\n",
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      " Answer: Berikut informasi yang saya temukan:\n",
      "\n",
      "\"alasan ketidaksuburan pada perempuan . selain itu , beberapa tikus betina lainnya juga menunjukkan menstruasi yang tidak rutin dan gangguan reproduksi lainnya . penelitian lain menyebutkan bahwa polycystic ovary syndrome mengakibatkan peningkatan hormon androgen hormon laki - laki pada perempuan , dan mengganggu ovarium untuk menghasilkan telur . b...\"\n",
      "\n",
      "(Sumber: Obesitas Mengurangi Kesuburan Wanita)\n",
      " Source: https://hellosehat.com/nutrisi/obesitas/kelebihan-berat-badan-mengurangi-kesuburan-wanita/\n",
      " Confidence: 284.33%\n",
      "  Intent: berat_badan, kesehatan_ibu, diabetes\n",
      "\n",
      " Question: Makanan apa yang baik untuk diet?\n",
      "============================================================\n",
      " Retrieving relevant documents...\n",
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      " Answer: Berikut informasi yang saya temukan:\n",
      "\n",
      "\"8 makanan setelah olahraga agar kembali berstamina . makan makanan setelah olahraga memang sangat diperlukan . pasalnya , aktivitas olahraga sangat melelahkan . sehingga , dibutuhkan asupan makanan dan minuman yang membantu menggantikan energi yang hilang . simak makanan apa saja yang disarankan untuk anda konsumsi sehabis berolahraga berikut ini !...\"\n",
      "\n",
      "(Sumber: 8 Makanan Setelah Olahraga agar Kembali Berstamina)\n",
      " Source: https://doktersehat.com/gaya-hidup/gizi-dan-nutrisi/makanan-setelah-olahraga/\n",
      " Confidence: 585.41%\n",
      "  Intent: fakta_gizi, pembentukan_tubuh, makanan_sehat\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TESTING\n",
    "# ============================================\n",
    "# Test queries\n",
    "test_questions = [\n",
    "    \"Apa penyebab perut buncit?\",\n",
    "    \"Bagaimana cara menurunkan berat badan?\",\n",
    "    \"Makanan apa yang baik untuk diet?\"\n",
    "]\n",
    "\n",
    "print(\"\\n Testing pipeline...\")\n",
    "for q in test_questions:\n",
    "    result = chatbot_pipeline(q)\n",
    "    print(f\"\\n Answer: {result['answer']}\")\n",
    "    print(f\" Source: {result['source']}\")\n",
    "    print(f\" Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"  Intent: {result['intent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775551cf-a2b1-434b-bd39-b02db7bc3923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "768a6c41-a2c4-4475-ac00-577a83a3813f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tahap 6: Evaluasi (model gagal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ad8de5-331a-42a5-b2f8-1dc0415652f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f46a6d-e61f-43ec-83b2-c2bf96f6c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52086c00-8328-46b9-b280-1338f8352a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bbfcdc-5f07-481c-a738-0018fec54e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading evaluation data...\n",
      "Test data: 151 paragraphs\n",
      "Total QA pairs: 394\n",
      "\n",
      "ü§ñ Loading QA Model...\n",
      "üìÇ Memuat ChromaDB Lokal dari: ./chroma_db_store ...\n",
      "‚úÖ Berhasil terhubung!\n",
      "üìä Jumlah Dokumen dalam Database: 7661\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD MODELS & DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"üìÇ Loading evaluation data...\")\n",
    "# Load test set dari QA dataset\n",
    "test_data = []\n",
    "with open('3_qa_dataset_strict_gold.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "# Ambil 20% terakhir sebagai test set\n",
    "split_idx = int(0.8 * len(test_data))\n",
    "test_data = test_data[split_idx:]\n",
    "print(f\"Test data: {len(test_data)} paragraphs\")\n",
    "\n",
    "# Hitung total QA pairs\n",
    "total_qa = sum(len(para['qas']) for para in test_data)\n",
    "print(f\"Total QA pairs: {total_qa}\")\n",
    "\n",
    "# Load chunks untuk retrieval\n",
    "df = pd.read_csv('output_dataset/processed_chunks.csv')\n",
    "\n",
    "# Load QA Model\n",
    "print(\"\\nü§ñ Loading QA Model...\")\n",
    "qa_model_path = './indobert-gizi-qa-final'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(qa_model_path)\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(qa_model_path)\n",
    "qa_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "qa_model.eval()\n",
    "\n",
    "# Load Retriever\n",
    "# --- KONFIGURASI PATH ---\n",
    "# Pastikan path ini SAMA PERSIS dengan tempat Anda menyimpan database sebelumnya\n",
    "CHROMA_DATA_PATH = \"./chroma_db_store\" \n",
    "COLLECTION_NAME = \"gizi_knowledge\"\n",
    "\n",
    "print(f\"üìÇ Memuat ChromaDB Lokal dari: {CHROMA_DATA_PATH} ...\")\n",
    "\n",
    "# 1. Cek apakah folder ada\n",
    "if not os.path.exists(CHROMA_DATA_PATH):\n",
    "    print(f\"‚ùå Error: Folder database '{CHROMA_DATA_PATH}' tidak ditemukan.\")\n",
    "    print(\"   Pastikan Anda sudah menjalankan proses indexing (add documents) setidaknya satu kali.\")\n",
    "else:\n",
    "    try:\n",
    "        # 2. Inisialisasi Persistent Client\n",
    "        chroma_client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "        # 3. Setup Embedding Function (WAJIB SAMA dengan saat pembuatan)\n",
    "        # Jika beda model, hasil pencarian akan ngaco.\n",
    "        embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "        )\n",
    "\n",
    "        # 4. Ambil Collection yang Sudah Ada\n",
    "        # Gunakan get_collection (bukan create_collection) untuk memastikan kita cuma loading\n",
    "        collection = chroma_client.get_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_func\n",
    "        )\n",
    "\n",
    "        # 5. Verifikasi Isi\n",
    "        count = collection.count()\n",
    "        print(f\"‚úÖ Berhasil terhubung!\")\n",
    "        print(f\"üìä Jumlah Dokumen dalam Database: {count}\")\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"‚ö†Ô∏è Peringatan: Collection ditemukan tapi KOSONG. Cek proses indexing Anda.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal memuat collection: {e}\")\n",
    "        print(\"   Tips: Cek apakah nama collection benar atau folder korup.\")\n",
    "\n",
    "# Load BM25\n",
    "tokenized_corpus = [doc.lower().split() for doc in df['text'].tolist()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6c259e-64a2-410c-93e8-a9849e6bf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RETRIEVAL FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def hybrid_retrieve(query, top_k=5):\n",
    "    dense_results = collection.query(query_texts=[query], n_results=top_k * 2)\n",
    "    \n",
    "    dense_docs = []\n",
    "    for i, doc_id in enumerate(dense_results['ids'][0]):\n",
    "        idx = int(doc_id.split('_')[1])\n",
    "        dense_docs.append({\n",
    "            'idx': idx,\n",
    "            'text': dense_results['documents'][0][i],\n",
    "            'dense_score': 1 / (1 + dense_results['distances'][0][i])\n",
    "        })\n",
    "    \n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[::-1][:top_k * 2]\n",
    "    \n",
    "    bm25_docs = {int(idx): float(bm25_scores[idx]) for idx in top_bm25_indices}\n",
    "    \n",
    "    combined = {}\n",
    "    for doc in dense_docs:\n",
    "        idx = doc['idx']\n",
    "        combined[idx] = {'text': doc['text'], 'score': 0.6 * doc['dense_score']}\n",
    "    \n",
    "    for idx, score in bm25_docs.items():\n",
    "        if idx in combined:\n",
    "            combined[idx]['score'] += 0.4 * score\n",
    "        else:\n",
    "            combined[idx] = {'text': df.iloc[idx]['text'], 'score': 0.4 * score}\n",
    "    \n",
    "    sorted_results = sorted(combined.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    return [{'text': item[1]['text'], 'score': item[1]['score']} for item in sorted_results[:top_k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7a8516-df20-4514-b5ec-1390eab937b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# QA FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def answer_question(question, context):\n",
    "    inputs = tokenizer(question, context, truncation=True, max_length=512, return_tensors='pt').to(qa_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = qa_model(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits[0]\n",
    "    end_logits = outputs.end_logits[0]\n",
    "    \n",
    "    start_probs = torch.softmax(start_logits, dim=0)\n",
    "    end_probs = torch.softmax(end_logits, dim=0)\n",
    "    \n",
    "    start_idx = torch.argmax(start_logits).item()\n",
    "    end_idx = torch.argmax(end_logits).item()\n",
    "    \n",
    "    confidence = (start_probs[start_idx] * end_probs[end_idx]).item()\n",
    "    \n",
    "    if start_idx >= end_idx:\n",
    "        return \"\", confidence\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    answer_tokens = tokens[start_idx:end_idx + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "    \n",
    "    return answer, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ea2b65-d1a8-4db5-976d-6f3d41922f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATION METRICS\n",
    "# ============================================\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Normalisasi teks untuk evaluasi\"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    \"\"\"Exact Match (EM)\"\"\"\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    \"\"\"F1 Score\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common = set(pred_tokens) & set(truth_tokens)\n",
    "    num_common = len(common)\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bb345f-6762-4170-882b-fd53837f662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä EVALUASI CHATBOT\n",
      "============================================================\n",
      "\n",
      "üîç Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:29<00:00,  5.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EVALUATION LOOP\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUASI CHATBOT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {\n",
    "    'retrieval': {'recall@1': [], 'recall@3': [], 'recall@5': []},\n",
    "    'reader': {'em': [], 'f1': [], 'confidence': []},\n",
    "    'threshold_analysis': {\n",
    "        'retriever': {'scores': [], 'has_answer': []},\n",
    "        'reader': {'scores': [], 'correct': []}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüîç Evaluating...\")\n",
    "for para in tqdm(test_data, desc=\"Processing\"):\n",
    "    context = para['context']\n",
    "    \n",
    "    for qa in para['qas']:\n",
    "        question = qa['question']\n",
    "        is_impossible = qa['is_impossible']\n",
    "        \n",
    "        # Skip unanswerable untuk evaluasi QA\n",
    "        if is_impossible:\n",
    "            continue\n",
    "        \n",
    "        ground_truth = qa['answers'][0]['text']\n",
    "        \n",
    "        # === EVALUASI RETRIEVAL ===\n",
    "        retrieved_docs = hybrid_retrieve(question, top_k=5)\n",
    "        \n",
    "        # Check if correct context retrieved\n",
    "        retrieved_texts = [doc['text'] for doc in retrieved_docs]\n",
    "        context_found_at = -1\n",
    "        \n",
    "        for i, doc_text in enumerate(retrieved_texts):\n",
    "            if normalize_answer(context) in normalize_answer(doc_text) or \\\n",
    "               normalize_answer(doc_text) in normalize_answer(context):\n",
    "                context_found_at = i\n",
    "                break\n",
    "        \n",
    "        results['retrieval']['recall@1'].append(1 if context_found_at == 0 else 0)\n",
    "        results['retrieval']['recall@3'].append(1 if 0 <= context_found_at < 3 else 0)\n",
    "        results['retrieval']['recall@5'].append(1 if 0 <= context_found_at < 5 else 0)\n",
    "        \n",
    "        # Threshold analysis - retriever\n",
    "        if retrieved_docs:\n",
    "            results['threshold_analysis']['retriever']['scores'].append(retrieved_docs[0]['score'])\n",
    "            results['threshold_analysis']['retriever']['has_answer'].append(context_found_at >= 0)\n",
    "        \n",
    "        # === EVALUASI READER ===\n",
    "        # Gunakan context asli untuk evaluasi reader\n",
    "        prediction, confidence = answer_question(question, context)\n",
    "        \n",
    "        # Compute metrics\n",
    "        em = compute_exact_match(prediction, ground_truth)\n",
    "        f1 = compute_f1(prediction, ground_truth)\n",
    "        \n",
    "        results['reader']['em'].append(em)\n",
    "        results['reader']['f1'].append(f1)\n",
    "        results['reader']['confidence'].append(confidence)\n",
    "        \n",
    "        # Threshold analysis - reader\n",
    "        results['threshold_analysis']['reader']['scores'].append(confidence)\n",
    "        results['threshold_analysis']['reader']['correct'].append(em or f1 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a45dbd-40cf-4384-957b-ceae6ba57803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà HASIL EVALUASI\n",
      "============================================================\n",
      "\n",
      "üéØ RETRIEVAL PERFORMANCE:\n",
      "  Recall@1: 32.10%\n",
      "  Recall@3: 45.68%\n",
      "  Recall@5: 51.85%\n",
      "\n",
      "üìñ READER PERFORMANCE:\n",
      "  Exact Match (EM): 19.75%\n",
      "  F1 Score: 29.54%\n",
      "  Avg Confidence: 14.39%\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# COMPUTE FINAL METRICS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà HASIL EVALUASI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ RETRIEVAL PERFORMANCE:\")\n",
    "print(f\"  Recall@1: {np.mean(results['retrieval']['recall@1']):.2%}\")\n",
    "print(f\"  Recall@3: {np.mean(results['retrieval']['recall@3']):.2%}\")\n",
    "print(f\"  Recall@5: {np.mean(results['retrieval']['recall@5']):.2%}\")\n",
    "\n",
    "print(\"\\nüìñ READER PERFORMANCE:\")\n",
    "print(f\"  Exact Match (EM): {np.mean(results['reader']['em']):.2%}\")\n",
    "print(f\"  F1 Score: {np.mean(results['reader']['f1']):.2%}\")\n",
    "print(f\"  Avg Confidence: {np.mean(results['reader']['confidence']):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3997f3de-ef4c-4a67-9432-1a1d03f290fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéöÔ∏è THRESHOLD ANALYSIS:\n",
      "\n",
      "  Retriever Score Distribution:\n",
      "    Min: 2.4894\n",
      "    25%: 6.2343\n",
      "    50%: 8.0632\n",
      "    75%: 10.0758\n",
      "    Max: 18.5546\n",
      "\n",
      "  Recommended Retriever Threshold: 16.8635\n",
      "    (Accuracy when passing threshold: 100.00%)\n",
      "\n",
      "  Reader Confidence Distribution:\n",
      "    Min: 0.0070\n",
      "    25%: 0.0316\n",
      "    50%: 0.0676\n",
      "    75%: 0.1515\n",
      "    Max: 0.9797\n",
      "\n",
      "  Recommended Reader Threshold: 0.4678\n",
      "    (Precision when passing threshold: 70.00%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# THRESHOLD ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüéöÔ∏è THRESHOLD ANALYSIS:\")\n",
    "\n",
    "# Retriever threshold\n",
    "retriever_scores = np.array(results['threshold_analysis']['retriever']['scores'])\n",
    "retriever_has_answer = np.array(results['threshold_analysis']['retriever']['has_answer'])\n",
    "\n",
    "print(\"\\n  Retriever Score Distribution:\")\n",
    "print(f\"    Min: {retriever_scores.min():.4f}\")\n",
    "print(f\"    25%: {np.percentile(retriever_scores, 25):.4f}\")\n",
    "print(f\"    50%: {np.percentile(retriever_scores, 50):.4f}\")\n",
    "print(f\"    75%: {np.percentile(retriever_scores, 75):.4f}\")\n",
    "print(f\"    Max: {retriever_scores.max():.4f}\")\n",
    "\n",
    "# Find optimal retriever threshold\n",
    "thresholds = np.linspace(retriever_scores.min(), retriever_scores.max(), 20)\n",
    "best_threshold = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for thresh in thresholds:\n",
    "    passed = retriever_scores >= thresh\n",
    "    if passed.sum() > 0:\n",
    "        accuracy = retriever_has_answer[passed].mean()\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = thresh\n",
    "\n",
    "print(f\"\\n  Recommended Retriever Threshold: {best_threshold:.4f}\")\n",
    "print(f\"    (Accuracy when passing threshold: {best_accuracy:.2%})\")\n",
    "\n",
    "# Reader threshold\n",
    "reader_scores = np.array(results['threshold_analysis']['reader']['scores'])\n",
    "reader_correct = np.array(results['threshold_analysis']['reader']['correct'])\n",
    "\n",
    "print(\"\\n  Reader Confidence Distribution:\")\n",
    "print(f\"    Min: {reader_scores.min():.4f}\")\n",
    "print(f\"    25%: {np.percentile(reader_scores, 25):.4f}\")\n",
    "print(f\"    50%: {np.percentile(reader_scores, 50):.4f}\")\n",
    "print(f\"    75%: {np.percentile(reader_scores, 75):.4f}\")\n",
    "print(f\"    Max: {reader_scores.max():.4f}\")\n",
    "\n",
    "# Find optimal reader threshold\n",
    "best_threshold = 0\n",
    "best_precision = 0\n",
    "\n",
    "for thresh in np.linspace(reader_scores.min(), reader_scores.max(), 20):\n",
    "    passed = reader_scores >= thresh\n",
    "    if passed.sum() > 0:\n",
    "        precision = reader_correct[passed].mean()\n",
    "        if precision > best_precision:\n",
    "            best_precision = precision\n",
    "            best_threshold = thresh\n",
    "\n",
    "print(f\"\\n  Recommended Reader Threshold: {best_threshold:.4f}\")\n",
    "print(f\"    (Precision when passing threshold: {best_precision:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df8764ea-b882-4f65-bc3e-367e9f59f3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Results saved to: evaluation_results.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "\n",
    "eval_summary = {\n",
    "    'retrieval': {\n",
    "        'recall@1': float(np.mean(results['retrieval']['recall@1'])),\n",
    "        'recall@3': float(np.mean(results['retrieval']['recall@3'])),\n",
    "        'recall@5': float(np.mean(results['retrieval']['recall@5']))\n",
    "    },\n",
    "    'reader': {\n",
    "        'exact_match': float(np.mean(results['reader']['em'])),\n",
    "        'f1_score': float(np.mean(results['reader']['f1'])),\n",
    "        'avg_confidence': float(np.mean(results['reader']['confidence']))\n",
    "    },\n",
    "    'recommended_thresholds': {\n",
    "        'retriever': float(best_threshold),\n",
    "        'reader': float(best_threshold)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nüíæ Results saved to: evaluation_results.json\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c4277-df4f-406c-aa65-b9e256011740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba3d7e8-3302-4d79-982b-c7d4f041f7ea",
   "metadata": {},
   "source": [
    "# Tahap 6: Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d234fd-a29e-4ae4-9370-6ee47a9367de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04e6ad8-4bea-493c-ae11-964438e3dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd4232e-2531-49c5-97f0-37fb64ed1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60cb4bc3-b4db-4c91-8201-3c6573da1da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading evaluation data...\n",
      "Test data: 151 paragraphs\n",
      "Total QA pairs: 394\n",
      "\n",
      "üîç Loading Retriever...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD MODELS & DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"üìÇ Loading evaluation data...\")\n",
    "# Load test set dari QA dataset\n",
    "test_data = []\n",
    "with open('3_qa_dataset_strict_gold.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "# Split 80/20 untuk test\n",
    "split_idx = int(0.8 * len(test_data))\n",
    "test_data = test_data[split_idx:]\n",
    "print(f\"Test data: {len(test_data)} paragraphs\")\n",
    "\n",
    "# Hitung total QA pairs\n",
    "total_qa = sum(len(para['qas']) for para in test_data)\n",
    "print(f\"Total QA pairs: {total_qa}\")\n",
    "\n",
    "# Load chunks untuk retrieval\n",
    "df = pd.read_csv('output_dataset/processed_chunks.csv')\n",
    "\n",
    "# Load Retriever\n",
    "print(\"\\nüîç Loading Retriever...\")\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_store\")\n",
    "collection = chroma_client.get_collection(\n",
    "    name=\"gizi_knowledge\",\n",
    "    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load BM25\n",
    "tokenized_corpus = [doc.lower().split() for doc in df['text'].tolist()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36fc9880-a7ba-42cc-95fb-942b4d0d981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RETRIEVAL FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def hybrid_retrieve(query, top_k=5):\n",
    "    dense_results = collection.query(query_texts=[query], n_results=top_k * 2)\n",
    "    \n",
    "    dense_docs = []\n",
    "    for i, doc_id in enumerate(dense_results['ids'][0]):\n",
    "        idx = int(doc_id.split('_')[1])\n",
    "        dense_docs.append({\n",
    "            'idx': idx,\n",
    "            'text': dense_results['documents'][0][i],\n",
    "            'dense_score': 1 / (1 + dense_results['distances'][0][i])\n",
    "        })\n",
    "    \n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[::-1][:top_k * 2]\n",
    "    \n",
    "    bm25_docs = {int(idx): float(bm25_scores[idx]) for idx in top_bm25_indices}\n",
    "    \n",
    "    combined = {}\n",
    "    for doc in dense_docs:\n",
    "        idx = doc['idx']\n",
    "        combined[idx] = {'text': doc['text'], 'score': 0.6 * doc['dense_score']}\n",
    "    \n",
    "    for idx, score in bm25_docs.items():\n",
    "        if idx in combined:\n",
    "            combined[idx]['score'] += 0.4 * score\n",
    "        else:\n",
    "            combined[idx] = {'text': df.iloc[idx]['text'], 'score': 0.4 * score}\n",
    "    \n",
    "    sorted_results = sorted(combined.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    return [{'text': item[1]['text'], 'score': item[1]['score'], 'idx': item[0]} \n",
    "            for item in sorted_results[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86eedb7f-1973-4f83-9432-650457d52f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATION METRICS\n",
    "# ============================================\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Normalisasi teks untuk evaluasi\"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    \"\"\"Exact Match (EM)\"\"\"\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    \"\"\"F1 Score based on token overlap\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common = set(pred_tokens) & set(truth_tokens)\n",
    "    num_common = len(common)\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def extract_answer_from_snippet(snippet, ground_truth):\n",
    "    \"\"\"\n",
    "    Cari ground truth dalam snippet\n",
    "    Jika ada, return True + F1 score\n",
    "    \"\"\"\n",
    "    snippet_norm = normalize_answer(snippet)\n",
    "    truth_norm = normalize_answer(ground_truth)\n",
    "    \n",
    "    # Check if ground truth exist in snippet\n",
    "    if truth_norm in snippet_norm:\n",
    "        return True, 1.0  # Perfect match\n",
    "    \n",
    "    # Calculate token overlap F1\n",
    "    f1 = compute_f1(snippet, ground_truth)\n",
    "    \n",
    "    # Threshold: F1 > 0.3 dianggap berhasil\n",
    "    return (f1 > 0.3), f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9e523b2-65b8-4a02-9091-109702194954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä EVALUASI SEARCH ENGINE\n",
      "============================================================\n",
      "\n",
      "üîç Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:18<00:00,  8.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EVALUATION LOOP\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUASI SEARCH ENGINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {\n",
    "    'retrieval': {\n",
    "        'recall@1': [],\n",
    "        'recall@3': [],\n",
    "        'recall@5': [],\n",
    "        'mrr': []  # Mean Reciprocal Rank\n",
    "    },\n",
    "    'answer_quality': {\n",
    "        'em': [],\n",
    "        'f1': [],\n",
    "        'has_answer': []\n",
    "    },\n",
    "    'confidence': {\n",
    "        'scores': [],\n",
    "        'correct': []\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüîç Evaluating...\")\n",
    "for para in tqdm(test_data, desc=\"Processing\"):\n",
    "    context = para['context']\n",
    "    \n",
    "    for qa in para['qas']:\n",
    "        question = qa['question']\n",
    "        is_impossible = qa['is_impossible']\n",
    "        \n",
    "        # Skip unanswerable untuk evaluasi\n",
    "        if is_impossible:\n",
    "            continue\n",
    "        \n",
    "        ground_truth = qa['answers'][0]['text']\n",
    "        \n",
    "        # === EVALUASI RETRIEVAL ===\n",
    "        retrieved_docs = hybrid_retrieve(question, top_k=5)\n",
    "        \n",
    "        # Check if correct context retrieved\n",
    "        context_found_at = -1\n",
    "        context_norm = normalize_answer(context)\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            doc_norm = normalize_answer(doc['text'])\n",
    "            # Check overlap (flexible matching)\n",
    "            if context_norm in doc_norm or doc_norm in context_norm:\n",
    "                context_found_at = i\n",
    "                break\n",
    "        \n",
    "        # Recall@k\n",
    "        results['retrieval']['recall@1'].append(1 if context_found_at == 0 else 0)\n",
    "        results['retrieval']['recall@3'].append(1 if 0 <= context_found_at < 3 else 0)\n",
    "        results['retrieval']['recall@5'].append(1 if 0 <= context_found_at < 5 else 0)\n",
    "        \n",
    "        # MRR (Mean Reciprocal Rank)\n",
    "        if context_found_at >= 0:\n",
    "            results['retrieval']['mrr'].append(1 / (context_found_at + 1))\n",
    "        else:\n",
    "            results['retrieval']['mrr'].append(0)\n",
    "        \n",
    "        # === EVALUASI ANSWER QUALITY ===\n",
    "        # Gunakan top-1 retrieved document sebagai \"answer\"\n",
    "        if retrieved_docs:\n",
    "            top_snippet = retrieved_docs[0]['text'][:400]  # Snippet yang akan ditampilkan\n",
    "            top_score = retrieved_docs[0]['score']\n",
    "            \n",
    "            # Check if ground truth ada di snippet\n",
    "            has_answer, f1_score = extract_answer_from_snippet(top_snippet, ground_truth)\n",
    "            \n",
    "            results['answer_quality']['has_answer'].append(has_answer)\n",
    "            results['answer_quality']['f1'].append(f1_score)\n",
    "            \n",
    "            # EM: strict match (jarang terjadi di snippet mode, tapi tetap dihitung)\n",
    "            em = compute_exact_match(top_snippet, ground_truth)\n",
    "            results['answer_quality']['em'].append(em)\n",
    "            \n",
    "            # === CONFIDENCE ANALYSIS ===\n",
    "            results['confidence']['scores'].append(top_score)\n",
    "            results['confidence']['correct'].append(has_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d49871-6ec0-4eb3-9631-13535abe5bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà HASIL EVALUASI\n",
      "============================================================\n",
      "\n",
      "üéØ RETRIEVAL PERFORMANCE:\n",
      "  Recall@1: 32.10%\n",
      "  Recall@3: 45.68%\n",
      "  Recall@5: 51.85%\n",
      "  MRR (Mean Reciprocal Rank): 0.3965\n",
      "\n",
      "üìñ ANSWER QUALITY (Search Engine Mode):\n",
      "  Answer Found Rate: 10.70%\n",
      "  Average F1 Score: 14.59%\n",
      "  Exact Match (EM): 0.00%\n",
      "\n",
      "üìä CONFIDENCE SCORE ANALYSIS:\n",
      "  Min Score: 2.4894\n",
      "  25th Percentile: 6.2343\n",
      "  Median: 8.0632\n",
      "  75th Percentile: 10.0758\n",
      "  Max Score: 18.5546\n",
      "\n",
      "üéöÔ∏è OPTIMAL THRESHOLD ANALYSIS:\n",
      "  Recommended Threshold: 7.5626\n",
      "    Precision at threshold: 16.67%\n",
      "    F1 Score at threshold: 12.07%\n",
      "  Correlation (Score vs Correctness): 0.1421\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# COMPUTE FINAL METRICS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà HASIL EVALUASI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ RETRIEVAL PERFORMANCE:\")\n",
    "print(f\"  Recall@1: {np.mean(results['retrieval']['recall@1']):.2%}\")\n",
    "print(f\"  Recall@3: {np.mean(results['retrieval']['recall@3']):.2%}\")\n",
    "print(f\"  Recall@5: {np.mean(results['retrieval']['recall@5']):.2%}\")\n",
    "print(f\"  MRR (Mean Reciprocal Rank): {np.mean(results['retrieval']['mrr']):.4f}\")\n",
    "\n",
    "print(\"\\nüìñ ANSWER QUALITY (Search Engine Mode):\")\n",
    "print(f\"  Answer Found Rate: {np.mean(results['answer_quality']['has_answer']):.2%}\")\n",
    "print(f\"  Average F1 Score: {np.mean(results['answer_quality']['f1']):.2%}\")\n",
    "print(f\"  Exact Match (EM): {np.mean(results['answer_quality']['em']):.2%}\")\n",
    "\n",
    "print(\"\\nüìä CONFIDENCE SCORE ANALYSIS:\")\n",
    "confidence_scores = np.array(results['confidence']['scores'])\n",
    "confidence_correct = np.array(results['confidence']['correct'])\n",
    "\n",
    "print(f\"  Min Score: {confidence_scores.min():.4f}\")\n",
    "print(f\"  25th Percentile: {np.percentile(confidence_scores, 25):.4f}\")\n",
    "print(f\"  Median: {np.percentile(confidence_scores, 50):.4f}\")\n",
    "print(f\"  75th Percentile: {np.percentile(confidence_scores, 75):.4f}\")\n",
    "print(f\"  Max Score: {confidence_scores.max():.4f}\")\n",
    "\n",
    "# Find optimal threshold\n",
    "print(\"\\nüéöÔ∏è OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "thresholds = np.linspace(confidence_scores.min(), confidence_scores.max(), 20)\n",
    "best_threshold = 0\n",
    "best_precision = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for thresh in thresholds:\n",
    "    passed = confidence_scores >= thresh\n",
    "    if passed.sum() > 0:\n",
    "        precision = confidence_correct[passed].mean()\n",
    "        recall = confidence_correct[passed].sum() / len(confidence_correct)\n",
    "        \n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = thresh\n",
    "                best_precision = precision\n",
    "\n",
    "print(f\"  Recommended Threshold: {best_threshold:.4f}\")\n",
    "print(f\"    Precision at threshold: {best_precision:.2%}\")\n",
    "print(f\"    F1 Score at threshold: {best_f1:.2%}\")\n",
    "\n",
    "# Correlation between confidence and correctness\n",
    "correlation = np.corrcoef(confidence_scores, confidence_correct)[0, 1]\n",
    "print(f\"  Correlation (Score vs Correctness): {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e26fe629-59a9-43bc-8ed8-087ad5728ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã CONFIDENCE SCORE BUCKETS:\n",
      "  Very High (2.0-10.0): 180 samples, Accuracy: 8.33%\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DETAILED BREAKDOWN\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüìã CONFIDENCE SCORE BUCKETS:\")\n",
    "buckets = [\n",
    "    (0.0, 0.5, \"Very Low\"),\n",
    "    (0.5, 1.0, \"Low\"),\n",
    "    (1.0, 1.5, \"Medium\"),\n",
    "    (1.5, 2.0, \"High\"),\n",
    "    (2.0, 10.0, \"Very High\")\n",
    "]\n",
    "\n",
    "for low, high, label in buckets:\n",
    "    mask = (confidence_scores >= low) & (confidence_scores < high)\n",
    "    count = mask.sum()\n",
    "    if count > 0:\n",
    "        accuracy = confidence_correct[mask].mean()\n",
    "        print(f\"  {label} ({low:.1f}-{high:.1f}): {count} samples, Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d096d6f-c456-48c9-9766-22ae85c4ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Results saved to: evaluation_results.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "\n",
    "eval_summary = {\n",
    "    'retrieval': {\n",
    "        'recall@1': float(np.mean(results['retrieval']['recall@1'])),\n",
    "        'recall@3': float(np.mean(results['retrieval']['recall@3'])),\n",
    "        'recall@5': float(np.mean(results['retrieval']['recall@5'])),\n",
    "        'mrr': float(np.mean(results['retrieval']['mrr']))\n",
    "    },\n",
    "    'answer_quality': {\n",
    "        'answer_found_rate': float(np.mean(results['answer_quality']['has_answer'])),\n",
    "        'average_f1': float(np.mean(results['answer_quality']['f1'])),\n",
    "        'exact_match': float(np.mean(results['answer_quality']['em']))\n",
    "    },\n",
    "    'confidence': {\n",
    "        'min': float(confidence_scores.min()),\n",
    "        'median': float(np.median(confidence_scores)),\n",
    "        'max': float(confidence_scores.max()),\n",
    "        'correlation': float(correlation),\n",
    "        'optimal_threshold': float(best_threshold),\n",
    "        'precision_at_threshold': float(best_precision)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nüíæ Results saved to: evaluation_results.json\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4d313-ec5c-4315-bc38-d04faf21f5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
