{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5670a488-7d78-4236-809a-17c42b29ecff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre-Pipeline: Import and combine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e01e31e-4096-4278-afd3-5f0c2b5aa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('original_dataset/hellosehat_dataset_10.csv', sep=';', encoding='utf-8')\n",
    "df2 = pd.read_csv('original_dataset/alodokter_dataset_10.csv', sep=';', encoding='utf-8')\n",
    "df3 = pd.read_csv('original_dataset/doktersehat_gizi_final_10.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45fe201-1e73-40a7-a122-178c1278ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dokumen setelah concat: 2391\n",
      "Index range: 0 - 2390\n",
      "Kolom: ['URL', 'Judul', 'Konten']\n"
     ]
    }
   ],
   "source": [
    "df_combine = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(f\"Total dokumen setelah concat: {len(df_combine)}\")\n",
    "print(f\"Index range: {df_combine.index.min()} - {df_combine.index.max()}\")\n",
    "print(f\"Kolom: {df_combine.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0983142-15de-423f-bfe9-856d8eec8d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Judul</th>\n",
       "      <th>Konten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Kenali 9 Penyebab Perut Buncit dan Cara Mengat...</td>\n",
       "      <td>Perut buncit memang mampu memengaruhi penampil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hellosehat.com/nutrisi/tips-makan-seha...</td>\n",
       "      <td>8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue</td>\n",
       "      <td>Bagi Anda yang gemar bikin kue, oven gas menja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hellosehat.com/nutrisi/fakta-gizi/mere...</td>\n",
       "      <td>10 Merek Oatmeal yang Bergizi dan Cocok untuk ...</td>\n",
       "      <td>Butuh menu sarapan yang cepat? Berbagai merek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hellosehat.com/nutrisi/resep-sehat/jus...</td>\n",
       "      <td>4 Resep Jus untuk Bantu Meningkatkan Sistem Im...</td>\n",
       "      <td>Setiap harinya, sistem imunitas pada tubuh bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Apakah Sering Buang Air Bisa Menurunkan Berat ...</td>\n",
       "      <td>Setelah diolah, dicerna, dan diambil semua giz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "1  https://hellosehat.com/nutrisi/tips-makan-seha...   \n",
       "2  https://hellosehat.com/nutrisi/fakta-gizi/mere...   \n",
       "3  https://hellosehat.com/nutrisi/resep-sehat/jus...   \n",
       "4  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "\n",
       "                                               Judul  \\\n",
       "0  Kenali 9 Penyebab Perut Buncit dan Cara Mengat...   \n",
       "1    8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue   \n",
       "2  10 Merek Oatmeal yang Bergizi dan Cocok untuk ...   \n",
       "3  4 Resep Jus untuk Bantu Meningkatkan Sistem Im...   \n",
       "4  Apakah Sering Buang Air Bisa Menurunkan Berat ...   \n",
       "\n",
       "                                              Konten  \n",
       "0  Perut buncit memang mampu memengaruhi penampil...  \n",
       "1  Bagi Anda yang gemar bikin kue, oven gas menja...  \n",
       "2  Butuh menu sarapan yang cepat? Berbagai merek ...  \n",
       "3  Setiap harinya, sistem imunitas pada tubuh bek...  \n",
       "4  Setelah diolah, dicerna, dan diambil semua giz...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d15e7e-91e5-4582-b503-b4e902ad18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX that thang!\n",
    "# df_combine.to_csv('output_dataset/combined_nutrition_dataset_2.csv', index=False, encoding='utf-8', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240d6b6f-ee09-46f7-8ef3-12d7e0e96db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Judul</th>\n",
       "      <th>Konten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Kenali 9 Penyebab Perut Buncit dan Cara Mengat...</td>\n",
       "      <td>Perut buncit memang mampu memengaruhi penampil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hellosehat.com/nutrisi/tips-makan-seha...</td>\n",
       "      <td>8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue</td>\n",
       "      <td>Bagi Anda yang gemar bikin kue, oven gas menja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hellosehat.com/nutrisi/fakta-gizi/mere...</td>\n",
       "      <td>10 Merek Oatmeal yang Bergizi dan Cocok untuk ...</td>\n",
       "      <td>Butuh menu sarapan yang cepat? Berbagai merek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hellosehat.com/nutrisi/resep-sehat/jus...</td>\n",
       "      <td>4 Resep Jus untuk Bantu Meningkatkan Sistem Im...</td>\n",
       "      <td>Setiap harinya, sistem imunitas pada tubuh bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Apakah Sering Buang Air Bisa Menurunkan Berat ...</td>\n",
       "      <td>Setelah diolah, dicerna, dan diambil semua giz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "1  https://hellosehat.com/nutrisi/tips-makan-seha...   \n",
       "2  https://hellosehat.com/nutrisi/fakta-gizi/mere...   \n",
       "3  https://hellosehat.com/nutrisi/resep-sehat/jus...   \n",
       "4  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "\n",
       "                                               Judul  \\\n",
       "0  Kenali 9 Penyebab Perut Buncit dan Cara Mengat...   \n",
       "1    8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue   \n",
       "2  10 Merek Oatmeal yang Bergizi dan Cocok untuk ...   \n",
       "3  4 Resep Jus untuk Bantu Meningkatkan Sistem Im...   \n",
       "4  Apakah Sering Buang Air Bisa Menurunkan Berat ...   \n",
       "\n",
       "                                              Konten  \n",
       "0  Perut buncit memang mampu memengaruhi penampil...  \n",
       "1  Bagi Anda yang gemar bikin kue, oven gas menja...  \n",
       "2  Butuh menu sarapan yang cepat? Berbagai merek ...  \n",
       "3  Setiap harinya, sistem imunitas pada tubuh bek...  \n",
       "4  Setelah diolah, dicerna, dan diambil semua giz...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('output_dataset/combined_nutrition_dataset.csv', sep=';', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad3623e6-a206-41f4-855c-2f3735e0fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL       https://doktersehat.com/gaya-hidup/gizi-dan-nu...\n",
      "Judul        Sumber, Manfaat, dan Dampak Kekurangan Omega-3\n",
      "Konten     Kita selalu menganggap kalau omega-3 adalah n...\n",
      "Name: 2150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "row = df.loc[2150]\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef974a-662b-448a-aea8-f8f33c409842",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre-Pipeline: Intent Tagging (Health Goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddde1b5b-86f7-4c8d-9797-fb69ff3c761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1008306-2c1f-46f1-a3b8-1140ec3bd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. KONFIGURASI INPUT & OUTPUT ---\n",
    "INPUT_FILE = 'output_dataset/combined_nutrition_dataset.csv'\n",
    "OUTPUT_FILE = 'output_dataset/tagged_combined_nutrition_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7d2538-55a2-4ad3-9190-b7ec15f0b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. KAMUS KEYWORD INTENT ---\n",
    "INTENT_KEYWORDS = {\n",
    "    'diabetes': ['diabetes', 'gula darah', 'kencing manis', 'insulin', 'glukosa', 'hiperglikemia'],\n",
    "    'anemia': ['anemia', 'kurang darah', 'zat besi', 'hemoglobin', 'pucat', 'lelah'],\n",
    "    'kesehatan_ibu': ['hamil', 'menyusui', 'bumil', 'busui', 'asi', 'janin', 'kandungan', 'kehamilan'],\n",
    "    'kesehatan_anak': ['anak', 'bayi', 'balita', 'si kecil', 'tumbuh kembang', 'imunisasi'],\n",
    "    'berat_badan': ['berat badan', 'diet', 'kurus', 'gemuk', 'langsing', 'turun berat', 'buncit', 'lemak', 'kalori', 'obesitas'],\n",
    "    'pembentukan_tubuh': ['otot', 'gym', 'fitness', 'binaraga', 'sixpack', 'latihan beban', 'workout', 'massa otot'],\n",
    "    'kesehatan_pencernaan': ['pencernaan', 'usus', 'lambung', 'maag', 'gerd', 'sembelit', 'diare', 'serat'],\n",
    "    'resep_sehat': ['resep', 'cara membuat', 'bahan-bahan', 'menu masakan', 'cara masak', 'hidangan'],\n",
    "    'diet_khusus': ['keto', 'vegan', 'vegetarian', 'gluten free', 'rendah garam', 'dash diet', 'intermittent'],\n",
    "    'pencegahan': ['mencegah', 'risiko', 'hindari', 'bahaya', 'waspada', 'gejala', 'tanda-tanda'],\n",
    "    'fakta_gizi': ['kandungan gizi', 'nutrisi', 'protein', 'karbohidrat', 'vitamin', 'mineral', 'takaran saji'],\n",
    "    'makanan_sehat': ['buah', 'sayur', 'organik', 'superfood', 'makanan sehat', 'bijian', 'kacang'],\n",
    "    # Fallback\n",
    "    'Kesehatan_umum': ['manfaat', 'khasiat', 'sehat', 'bugar', 'stamina', 'daya tahan', 'imun', 'kesehatan']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c96a7416-7d41-4040-aa40-11eebc72edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_3_intents(text):\n",
    "    \"\"\"\n",
    "    Menghitung frekuensi keyword dan mengembalikan maksimal 3 intent teratas.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for intent, keywords in INTENT_KEYWORDS.items():\n",
    "        count = 0\n",
    "        for kw in keywords:\n",
    "            count += text.count(kw)\n",
    "        if count > 0:\n",
    "            scores[intent] = count\n",
    "    \n",
    "    # Jika tidak ada match, return default\n",
    "    if not scores:\n",
    "        return \"Kesehatan_umum\"\n",
    "    \n",
    "    # Urutkan score tertinggi -> terendah\n",
    "    sorted_intents = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Ambil 3 teratas\n",
    "    top_3 = [item[0] for item in sorted_intents[:3]]\n",
    "    return \", \".join(top_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "288ce2ad-f4fa-4ee9-af29-702e23b8ef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Membaca file input: output_dataset/combined_nutrition_dataset.csv ...\n",
      "üìä Total Data: 2425 baris.\n",
      "üîç Melakukan Auto-Tagging Intent...\n",
      "üíæ Menyimpan ke format final (Comma Separated + Quoted)...\n",
      "‚úÖ SUKSES! File tersimpan: output_dataset/tagged_combined_nutrition_dataset.csv\n",
      "   Format: \"URL\",\"Title\",\"Content\",\"Intent\" (Comma Separated)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. EKSEKUSI UTAMA ---\n",
    "def process_tagging():\n",
    "    print(f\"üìÇ Membaca file input: {INPUT_FILE} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Membaca file dengan separator ';'\n",
    "        # Kita asumsikan baris pertama adalah header [URL;Judul;Konten]\n",
    "        df = pd.read_csv(INPUT_FILE, sep=';', on_bad_lines='skip')\n",
    "        \n",
    "        # Validasi kolom dasar (Sesuaikan nama kolom jika di file Anda berbeda)\n",
    "        # Kita rename standar agar mudah diproses\n",
    "        # Asumsi urutan kolom: 1. URL, 2. Judul, 3. Konten\n",
    "        if len(df.columns) >= 3:\n",
    "            df.columns = ['url', 'title', 'content'] + list(df.columns[3:])\n",
    "        else:\n",
    "            print(\"‚ùå Error: File input harus memiliki minimal 3 kolom (URL;Judul;Konten)\")\n",
    "            return\n",
    "\n",
    "        print(f\"üìä Total Data: {len(df)} baris.\")\n",
    "        \n",
    "        # Bersihkan data (isi yang kosong dengan string kosong)\n",
    "        df['title'] = df['title'].fillna('').astype(str)\n",
    "        df['content'] = df['content'].fillna('').astype(str)\n",
    "        \n",
    "        print(\"üîç Melakukan Auto-Tagging Intent...\")\n",
    "        # Gabungkan Judul + Konten untuk pencarian keyword yang lebih akurat\n",
    "        df['full_text_scan'] = df['title'] + \" \" + df['content']\n",
    "        df['intent'] = df['full_text_scan'].apply(get_top_3_intents)\n",
    "        \n",
    "        # Hapus kolom bantuan\n",
    "        df.drop(columns=['full_text_scan'], inplace=True)\n",
    "        \n",
    "        print(f\"üíæ Menyimpan ke format final (Comma Separated + Quoted)...\")\n",
    "        \n",
    "        # --- TEKNIK PENYIMPANAN PENTING ---\n",
    "        # quoting=csv.QUOTE_ALL : Memaksa SEMUA kolom dibungkus tanda kutip \"...\"\n",
    "        # Ini menjamin koma di dalam teks TIDAK akan dianggap sebagai pemisah kolom baru.\n",
    "        df.to_csv(\n",
    "            OUTPUT_FILE, \n",
    "            sep=',',              # Separator Koma\n",
    "            quotechar='\"',        # Pembungkus Tanda Kutip Ganda\n",
    "            quoting=csv.QUOTE_ALL, # MODE AMAN: Bungkus semua data dengan kutip\n",
    "            index=False,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ SUKSES! File tersimpan: {OUTPUT_FILE}\")\n",
    "        print(\"   Format: \\\"URL\\\",\\\"Title\\\",\\\"Content\\\",\\\"Intent\\\" (Comma Separated)\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {INPUT_FILE} tidak ditemukan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Terjadi kesalahan: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2710d3-660b-44e8-a980-bb0db5dbbe54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tahap 1: Preprocessing and chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51677a25-e972-4303-8bc2-423413df53a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1a411-75b5-49bd-883f-ec83ea4b04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('output_dataset/tagged_combined_nutrition_dataset.csv')\n",
    "print(f\"Total dokumen: {len(df)}\")\n",
    "print(f\"Kolom dataset: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140e8fc-504b-49f8-894d-3829b083b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IndoBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Normalisasi teks\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s.,!?%-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Fungsi chunking dengan sliding window\n",
    "def chunk_text_with_overlap(text, max_tokens=384, overlap=50):\n",
    "    \"\"\"\n",
    "    Chunking dengan overlap sliding window\n",
    "    max_tokens: 512 - 128 (buffer untuk question) = 384\n",
    "    overlap: 50 tokens\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a478aa-4ed5-4d65-b52a-882228adaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proses semua dokumen\n",
    "processed_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    title = row.get('title', '')\n",
    "    content = row.get('content', '')\n",
    "    url = row.get('url', '')\n",
    "    intent = row.get('intent', '')\n",
    "    \n",
    "    full_text = f\"{title}. {content}\".strip()\n",
    "    cleaned_text = preprocess_text(full_text)\n",
    "    \n",
    "    if not cleaned_text:\n",
    "        continue\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(cleaned_text)\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        processed_data.append({\n",
    "            'doc_id': idx,\n",
    "            'chunk_id': chunk_idx,\n",
    "            'title': title,\n",
    "            'text': chunk,\n",
    "            'url': url,\n",
    "            'intent': intent,\n",
    "            'token_count': len(tokenizer.tokenize(chunk))\n",
    "        })\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} documents\")\n",
    "\n",
    "# Simpan hasil\n",
    "df_processed = pd.DataFrame(processed_data)\n",
    "df_processed.to_csv('output_dataset/processed_chunks.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(df_processed)}\")\n",
    "print(f\"Rata-rata tokens per chunk: {df_processed['token_count'].mean():.2f}\")\n",
    "print(f\"Max tokens: {df_processed['token_count'].max()}\")\n",
    "print(f\"Min tokens: {df_processed['token_count'].min()}\")\n",
    "\n",
    "# Simpan corpus untuk MLM\n",
    "all_texts = df_processed['text'].tolist()\n",
    "with open('corpus_for_mlm.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nFile tersimpan:\")\n",
    "print(\"- processed_chunks.csv\")\n",
    "print(\"- corpus_for_mlm.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6a194-3675-4c7f-b714-77518f400d94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tahap 2: Domain ~~EXPANSION~~ Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd5d79-d6b7-416b-8c13-675e900ef7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForMaskedLM,\n",
    "    DataCollatorForWholeWordMask,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bf4a4b-67e9-4645-9e9e-eb4616503137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU Ditemukan: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   VRAM Total: 6.44 GB\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP GPU & CLEAR MEMORY ---\n",
    "# Cek apakah CUDA (GPU) tersedia\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"‚úÖ GPU Ditemukan: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Bersihkan cache memori sebelumnya (Penting jika menggunakan Notebook)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"GPU tidak ditemukan! Menggunakan CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e1d0ce-a40d-4eb2-ba1b-667baa1a87df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts: 7661\n",
      "Data Training: 6894\n",
      "Data Validasi: 767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. LOAD DATA ---\n",
    "# Load corpus\n",
    "try:\n",
    "    with open('corpus_for_mlm.json', 'r', encoding='utf-8') as f:\n",
    "        all_texts = json.load(f)\n",
    "    print(f\"Total texts: {len(all_texts)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File corpus tidak ditemukan, menggunakan dummy data.\")\n",
    "    all_texts = [\"1\", \"2\"]\n",
    "\n",
    "# SPLIT DATA (90% Train, 10% Validation)\n",
    "# Kita butuh ini agar model bisa diuji (Val Loss muncul)\n",
    "split_idx = int(0.9 * len(all_texts))\n",
    "train_texts = all_texts[:split_idx]\n",
    "val_texts = all_texts[split_idx:]\n",
    "\n",
    "print(f\"Data Training: {len(train_texts)}\")\n",
    "print(f\"Data Validasi: {len(val_texts)}\")\n",
    "\n",
    "# Load model dan tokenizer\n",
    "model_name = 'indobenchmark/indobert-base-p2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d03145-5895-4d52-a9c2-63fba39f1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mempersiapkan Training Set...\n",
      "Preprocessing texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274fa7e28e6d44b984c4c94daa78d773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/6894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mempersiapkan Validation Set...\n",
      "Preprocessing texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26feedc38a243c6939aacb9e9eb96ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\data\\data_collator.py:1642: FutureWarning: DataCollatorForWholeWordMask is deprecated and will be removed in a future version, you can now use DataCollatorForLanguageModeling with whole_word_mask=True instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 3. PINDAHKAN MODEL KE GPU ---\n",
    "model.to(device)\n",
    "\n",
    "# Custom Dataset untuk MLM\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Preprocess dengan progress bar\n",
    "        print(\"Preprocessing texts...\")\n",
    "        self.encodings = []\n",
    "        for text in tqdm(texts, desc=\"Tokenizing\"):\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.encodings.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze()\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings[idx]\n",
    "\n",
    "# Custom Data Collator\n",
    "class CustomWWMDataCollator(DataCollatorForWholeWordMask):\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15):\n",
    "        super().__init__(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=True,\n",
    "            mlm_probability=mlm_probability\n",
    "        )\n",
    "    \n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask=None):\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        \n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "                for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "        \n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        \n",
    "        for i, input_ids in enumerate(inputs):\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            for j, token in enumerate(tokens):\n",
    "                if re.search(r'\\d', token) or token in ['mg', 'gram', 'kg', 'ml', 'kkal', 'kalori', '%']:\n",
    "                    probability_matrix[i, j] = 0.0\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "        return inputs, labels\n",
    "\n",
    "# Buat 2 Dataset Terpisah\n",
    "print(\"\\nMempersiapkan Training Set...\")\n",
    "train_dataset = MLMDataset(train_texts, tokenizer)\n",
    "\n",
    "print(\"Mempersiapkan Validation Set...\")\n",
    "val_dataset = MLMDataset(val_texts, tokenizer)\n",
    "\n",
    "# Data collator\n",
    "data_collator = CustomWWMDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f718a0-db49-4c58-a659-3812babd9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mulai Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2586' max='2586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2586/2586 46:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.563700</td>\n",
       "      <td>4.428384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.044200</td>\n",
       "      <td>3.764034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.597000</td>\n",
       "      <td>3.424848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.361500</td>\n",
       "      <td>3.217364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.164900</td>\n",
       "      <td>3.068305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.013400</td>\n",
       "      <td>2.934664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.841500</td>\n",
       "      <td>2.784796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.731400</td>\n",
       "      <td>2.662641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.627900</td>\n",
       "      <td>2.553934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.576000</td>\n",
       "      <td>2.442628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.479600</td>\n",
       "      <td>2.399022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.410200</td>\n",
       "      <td>2.328005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.376600</td>\n",
       "      <td>2.218874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.286200</td>\n",
       "      <td>2.147584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.237600</td>\n",
       "      <td>2.087961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.122400</td>\n",
       "      <td>2.026285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.082000</td>\n",
       "      <td>1.984129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.026600</td>\n",
       "      <td>1.909266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.987600</td>\n",
       "      <td>1.892458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.956700</td>\n",
       "      <td>1.861038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.945200</td>\n",
       "      <td>1.850937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.940900</td>\n",
       "      <td>1.826260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.896500</td>\n",
       "      <td>1.831095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.881300</td>\n",
       "      <td>1.827258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.837200</td>\n",
       "      <td>1.797632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan model...\n",
      "Selesai!\n"
     ]
    }
   ],
   "source": [
    "# --- 4. TRAINING ARGUMENTS ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./indobert-gizi-mlm',\n",
    "    overwrite_output_dir=True,\n",
    "      \n",
    "    # === HYPERPARAMETERS ===\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,              \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # === EVALUATION SETTINGS (BARU) ===\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    \n",
    "    # === LOGGING ===\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # === SYSTEM ===\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    no_cuda=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,       \n",
    "    disable_tqdm=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\nMulai Training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Simpan Model Akhir\n",
    "print(\"\\nMenyimpan model...\")\n",
    "model.save_pretrained(\"./indobert-gizi-mlm-final\")\n",
    "tokenizer.save_pretrained(\"./indobert-gizi-mlm-final\")\n",
    "print(\"Selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6cef1e-994c-47bf-b628-dfce7cd58aae",
   "metadata": {},
   "source": [
    "## Clear Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea5ee79-d37b-48d3-abfa-f113fed57604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc \n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c708a-5302-43ae-8d14-181084d2d112",
   "metadata": {},
   "source": [
    "# Tahap 3: Membuat Pasangan QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d96b67f4-1dda-4440-bd2d-af51cd56cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from groq import Groq, RateLimitError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df678427-6259-4cdd-a5c4-b8b794704d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. KONFIGURASI ---\n",
    "API_KEY = \"key \" \n",
    "client = Groq(api_key=API_KEY)\n",
    "\n",
    "# Model\n",
    "PRIMARY_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# File Paths (Sesuaikan dengan notebook Anda)\n",
    "INPUT_FILE = 'output_dataset/dataset_filtered_processed_chunks.csv'\n",
    "OUTPUT_FILE = '3_qa_dataset_strict_gold.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "286c92e8-318e-4202-ad56-f8347ef6d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. FUNGSI GENERATOR STRICT ---\n",
    "def generate_qa_strict(context):\n",
    "    prompt = f\"\"\"\n",
    "    You are a strict Data Annotator for a SQuAD dataset.\n",
    "    \n",
    "    TASK: Extract 3 QA pairs from the provided text.\n",
    "    \n",
    "    CRITICAL RULES:\n",
    "    1. The \"answer\" MUST be an EXACT SUBSTRING from the Context. Copy-paste ONLY. No paraphrasing.\n",
    "    2. Provide 2 answerable questions and 1 unanswerable question.\n",
    "    3. Language: Indonesian.\n",
    "    \n",
    "    CONTEXT:\n",
    "    \"{context}\"\n",
    "    \n",
    "    OUTPUT FORMAT (JSON ONLY):\n",
    "    [\n",
    "      {{ \"question\": \"Pertanyaan?\", \"answer\": \"exact substring\", \"is_impossible\": false }},\n",
    "      {{ \"question\": \"Pertanyaan sulit?\", \"answer\": \"\", \"is_impossible\": true }}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    max_retries = 5\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You output JSON only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                model=PRIMARY_MODEL,\n",
    "                temperature=0.0, # Wajib 0 agar tidak kreatif (konsisten)\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            # --- LOGIKA MENANGANI RATE LIMIT (429) ---\n",
    "            print(f\"‚ö†Ô∏è Rate Limit Hit! (Percobaan {retry_count + 1}/{max_retries})\")\n",
    "            \n",
    "            # Coba ambil informasi waktu tunggu dari header response\n",
    "            retry_after = 0\n",
    "            try:\n",
    "                headers = e.response.headers\n",
    "                \n",
    "                # Coba header standar 'retry-after'\n",
    "                if 'retry-after' in headers:\n",
    "                    retry_after = float(headers['retry-after'])\n",
    "                \n",
    "                # Coba header spesifik 'x-ratelimit-reset-tokens' (biasanya format \"12.34s\")\n",
    "                elif 'x-ratelimit-reset-tokens' in headers:\n",
    "                    reset_tokens = headers['x-ratelimit-reset-tokens']\n",
    "                    retry_after = float(reset_tokens.replace('s', ''))\n",
    "                \n",
    "                # Coba header spesifik 'x-ratelimit-reset-requests'\n",
    "                elif 'x-ratelimit-reset-requests' in headers:\n",
    "                    reset_reqs = headers['x-ratelimit-reset-requests']\n",
    "                    # Parse format \"12m30s\" atau \"12.34s\" jika perlu, tapi float simpel biasanya cukup untuk seconds\n",
    "                    if 'm' in reset_reqs: \n",
    "                        # Sederhana: jika ada menit, tunggu default 60 detik saja biar aman\n",
    "                        retry_after = 60 \n",
    "                    else:\n",
    "                        retry_after = float(reset_reqs.replace('s', ''))\n",
    "            \n",
    "            except Exception as header_err:\n",
    "                # Jika gagal parsing header, gunakan default\n",
    "                print(f\"   (Gagal baca header: {header_err})\")\n",
    "                retry_after = 20 # Default aman\n",
    "\n",
    "            # Pastikan retry_after minimal 1 detik & tambahkan buffer\n",
    "            wait_time = max(1, retry_after) + 2 \n",
    "            \n",
    "            print(f\"‚è≥ Tidur selama {wait_time:.2f} detik sebelum mencoba lagi...\")\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "            retry_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Error lain (misal koneksi putus, JSON malformed dari server, dll)\n",
    "            print(f\"‚ùå Error non-limit: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"‚ùå Gagal setelah max retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb2f12a6-7cc1-4ccb-97a4-cbd8c70da030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_strict_pipeline():\n",
    "    print(f\"‚öôÔ∏è Menyiapkan Pipeline Strict Mode dengan Model: {PRIMARY_MODEL}\")\n",
    "    \n",
    "    # Baca Data Input\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_FILE)\n",
    "        # Ambil kolom text, pastikan string\n",
    "        texts = df['text'].fillna('').astype(str).unique()\n",
    "        total_awal = len(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal baca input: {e}\")\n",
    "        return\n",
    "\n",
    "    # Logika Resume (Cek data yg sudah ada)\n",
    "    processed_contexts = set()\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"üìÇ Membaca checkpoint dari {OUTPUT_FILE}...\")\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    processed_contexts.add(data['context'])\n",
    "                except: continue\n",
    "    \n",
    "    # Filter data yang belum diproses\n",
    "    texts_to_process = [t for t in texts if t not in processed_contexts]\n",
    "    print(f\"üìä Sisa data: {len(texts_to_process)} dari {total_awal} total chunk unik.\")\n",
    "    \n",
    "    if not texts_to_process:\n",
    "        print(\"üéâ Semua data sudah selesai!\")\n",
    "        return\n",
    "\n",
    "    valid_count = 0\n",
    "    dropped_count = 0\n",
    "    \n",
    "    # Loop Utama\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "        for text in tqdm(texts_to_process, desc=\"Generating QA\"):\n",
    "            if len(text) < 50: continue\n",
    "            \n",
    "            # Request ke LLM (Pakai fungsi SMART)\n",
    "            qa_list = generate_qa_strict(text)\n",
    "            \n",
    "            # Parsing response\n",
    "            if qa_list:\n",
    "                # Handle variasi struktur JSON dari LLM\n",
    "                if isinstance(qa_list, dict): \n",
    "                    qa_list = qa_list.get('qas', qa_list.get('qa_pairs', []))\n",
    "                \n",
    "                squad_qas = []\n",
    "                # Pastikan qa_list adalah list sebelum iterasi\n",
    "                if isinstance(qa_list, list):\n",
    "                    for item in qa_list:\n",
    "                        # Validasi Format\n",
    "                        if not isinstance(item, dict): continue\n",
    "                        \n",
    "                        question = item.get('question')\n",
    "                        answer_text = item.get('answer', '')\n",
    "                        is_impossible = item.get('is_impossible', False)\n",
    "                        \n",
    "                        if is_impossible:\n",
    "                            squad_qas.append({\n",
    "                                \"id\": str(uuid.uuid4()),\n",
    "                                \"question\": question,\n",
    "                                \"answers\": [],\n",
    "                                \"is_impossible\": True\n",
    "                            })\n",
    "                        else:\n",
    "                            # --- STRICT CHECK (Validasi Substring) ---\n",
    "                            if not answer_text: continue\n",
    "                            \n",
    "                            # Case insensitive search untuk fleksibilitas sedikit\n",
    "                            # Tapi kita simpan text ASLI dari konteks\n",
    "                            start_idx = text.find(answer_text)\n",
    "                            \n",
    "                            if start_idx != -1:\n",
    "                                # LOLOS VALIDASI\n",
    "                                squad_qas.append({\n",
    "                                    \"id\": str(uuid.uuid4()),\n",
    "                                    \"question\": question,\n",
    "                                    \"answers\": [{\n",
    "                                        \"text\": answer_text,\n",
    "                                        \"answer_start\": start_idx\n",
    "                                    }],\n",
    "                                    \"is_impossible\": False\n",
    "                                })\n",
    "                                valid_count += 1\n",
    "                            else:\n",
    "                                # GAGAL VALIDASI (Halusinasi LLM) -> BUANG\n",
    "                                dropped_count += 1\n",
    "                \n",
    "                # Simpan jika ada QA valid\n",
    "                if squad_qas:\n",
    "                    entry = {\"context\": text, \"qas\": squad_qas}\n",
    "                    f.write(json.dumps(entry) + \"\\n\")\n",
    "                    f.flush()\n",
    "            \n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üèÅ PROSES SELESAI\")\n",
    "    print(f\"‚úÖ QA Valid Tersimpan: {valid_count}\")\n",
    "    print(f\"üóëÔ∏è QA Halusinasi Dibuang: {dropped_count}\")\n",
    "    print(f\"üìÇ Output: {OUTPUT_FILE}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52954aa1-7e4b-4be4-80e0-fbf5ab129979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Menyiapkan Pipeline Strict Mode dengan Model: llama-3.3-70b-versatile\n",
      "üìÇ Membaca checkpoint dari 3_qa_dataset_strict_gold.jsonl...\n",
      "üìä Sisa data: 699 dari 752 total chunk unik.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   0%|                                                                           | 0/699 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rate Limit Hit! (Percobaan 1/5)\n",
      "‚è≥ Tidur selama 541.00 detik sebelum mencoba lagi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   0%|                                                              | 1/699 [09:02<105:05:31, 542.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rate Limit Hit! (Percobaan 1/5)\n",
      "‚è≥ Tidur selama 692.00 detik sebelum mencoba lagi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   0%|‚ñè                                                             | 2/699 [20:34<122:07:03, 630.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rate Limit Hit! (Percobaan 1/5)\n",
      "‚è≥ Tidur selama 797.00 detik sebelum mencoba lagi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   0%|‚ñé                                                             | 3/699 [33:52<136:42:46, 707.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rate Limit Hit! (Percobaan 1/5)\n",
      "‚è≥ Tidur selama 802.00 detik sebelum mencoba lagi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   1%|‚ñé                                                             | 4/699 [47:15<143:48:31, 744.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rate Limit Hit! (Percobaan 1/5)\n",
      "‚è≥ Tidur selama 661.00 detik sebelum mencoba lagi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   1%|‚ñç                                                             | 5/699 [58:17<137:49:15, 714.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rate Limit Hit! (Percobaan 1/5)\n",
      "‚è≥ Tidur selama 744.00 detik sebelum mencoba lagi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   1%|‚ñå                                                           | 6/699 [1:10:42<139:34:49, 725.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rate Limit Hit! (Percobaan 1/5)\n",
      "‚è≥ Tidur selama 746.00 detik sebelum mencoba lagi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:   1%|‚ñå                                                           | 6/699 [1:23:08<160:02:46, 831.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mgenerate_qa_strict\u001b[39m\u001b[34m(context)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou output JSON only.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPRIMARY_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Wajib 0 agar tidak kreatif (konsisten)\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(completion.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1239\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1043\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kfj8y699fb6r8bx2802w99mt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100000, Requested 860. Please try again in 12m23.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Jalankan\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mrun_strict_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mrun_strict_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) < \u001b[32m50\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Request ke LLM (Pakai fungsi SMART)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m qa_list = \u001b[43mgenerate_qa_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Parsing response\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m qa_list:\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Handle variasi struktur JSON dari LLM\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mgenerate_qa_strict\u001b[39m\u001b[34m(context)\u001b[39m\n\u001b[32m     73\u001b[39m     wait_time = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, retry_after) + \u001b[32m2\u001b[39m \n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚è≥ Tidur selama \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m detik sebelum mencoba lagi...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     retry_count += \u001b[32m1\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# Error lain (misal koneksi putus, JSON malformed dari server, dll)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Jalankan\n",
    "if __name__ == \"__main__\":\n",
    "    run_strict_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b7537-6d6c-43fe-a8ab-d03b51157d20",
   "metadata": {},
   "source": [
    "# Tahap 4: Fine-Tuning QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fcedfd-43ee-4ad6-9fa2-ff54375f07bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import gc\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertModel, BertConfig \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0f69b0-d6d6-4c93-920f-0336569de1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   VRAM: 6.44 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU tidak tersedia, menggunakan CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1059711a-d032-4875-8488-f04d4562fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraf: 580\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = []\n",
    "with open('qa_dataset_final.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        qa_pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Total paragraf: {len(qa_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7a7ed8-a5d4-45b1-a900-c91fdee80c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Loading BERT encoder dari Domain Adaptation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./indobert-gizi-mlm-final and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model QA berhasil diinisialisasi.\n",
      "üîç Cek QA Outputs Layer: Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model dari Domain Adaptation\n",
    "model_path = './indobert-gizi-mlm-final'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Load base BERT dari MLM model, lalu inisialisasi QA head baru\n",
    "from transformers import BertConfig, BertForQuestionAnswering\n",
    "\n",
    "print(\"\\nüîß Loading BERT encoder dari Domain Adaptation...\")\n",
    "\n",
    "# === PERBAIKAN DI SINI ===\n",
    "# Kita paksa num_labels=2 agar output layer QA hanya 2 (Start & End)\n",
    "config = BertConfig.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Gunakan .from_pretrained langsung dengan ignore_mismatched_sizes=True\n",
    "# Ini akan memuat body BERT yang sudah di-training (MLM), tapi mereset head QA menjadi benar\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "    model_path, \n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model QA berhasil diinisialisasi.\")\n",
    "print(f\"üîç Cek QA Outputs Layer: {model.qa_outputs}\") \n",
    "# Pastikan output print di atas tertulis out_features=2\n",
    "\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ae9ec8-e46a-439c-9cff-e3945bae36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs: 1137\n"
     ]
    }
   ],
   "source": [
    "# Parse data ke format training\n",
    "train_examples = []\n",
    "\n",
    "for para in qa_pairs:\n",
    "    context = para['context']\n",
    "    for qa in para['qas']:\n",
    "        example = {\n",
    "            'id': qa['id'],\n",
    "            'question': qa['question'],\n",
    "            'context': context,\n",
    "        }\n",
    "        \n",
    "        if qa['is_impossible']:\n",
    "            example['answers'] = {'answer_start': [], 'text': []}\n",
    "        else:\n",
    "            example['answers'] = {\n",
    "                'answer_start': [qa['answers'][0]['answer_start']],\n",
    "                'text': [qa['answers'][0]['text']]\n",
    "            }\n",
    "        \n",
    "        train_examples.append(example)\n",
    "\n",
    "print(f\"Total QA pairs: {len(train_examples)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95322459-3425-410f-bc7e-fc2f67a66250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1023\n",
      "Validation: 114\n"
     ]
    }
   ],
   "source": [
    "# Split train/validation (90/10)\n",
    "split_idx = int(0.9 * len(train_examples))\n",
    "train_data = train_examples[:split_idx]\n",
    "val_data = train_examples[split_idx:]\n",
    "\n",
    "print(f\"Training: {len(train_data)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16fe8ce9-da45-42c4-9511-4b9ec8b8e29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Tokenizing training data...\n",
      "üîÑ Tokenizing validation data...\n"
     ]
    }
   ],
   "source": [
    "# Tokenisasi dengan batch processing\n",
    "def tokenize_dataset(examples):\n",
    "    questions = [ex['question'] for ex in examples]\n",
    "    contexts = [ex['context'] for ex in examples]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation='only_second',\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
    "    offset_mapping = tokenized['offset_mapping']\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized['input_ids'][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[sample_index]['answers']\n",
    "        \n",
    "        # Unanswerable question\n",
    "        if len(answers['answer_start']) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "        \n",
    "        start_char = answers['answer_start'][0]\n",
    "        end_char = start_char + len(answers['text'][0])\n",
    "        \n",
    "        # Find token start position\n",
    "        token_start_index = 0\n",
    "        while token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        \n",
    "        # Find token end position\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while token_end_index >= 0 and sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        # Check if answer is in this chunk\n",
    "        if not (token_start_index < len(offsets) and \n",
    "                token_end_index < len(offsets) and\n",
    "                offsets[token_start_index][0] <= start_char and \n",
    "                offsets[token_end_index][1] >= end_char):\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "        \n",
    "        # Find exact token positions\n",
    "        while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "            token_start_index += 1\n",
    "        start_positions.append(token_start_index - 1)\n",
    "        \n",
    "        while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "            token_end_index -= 1\n",
    "        end_positions.append(token_end_index + 1)\n",
    "    \n",
    "    tokenized['start_positions'] = start_positions\n",
    "    tokenized['end_positions'] = end_positions\n",
    "    tokenized.pop('offset_mapping')\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"\\nüîÑ Tokenizing training data...\")\n",
    "tokenized_train = tokenize_dataset(train_data)\n",
    "\n",
    "print(\"üîÑ Tokenizing validation data...\")\n",
    "tokenized_val = tokenize_dataset(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e18169e-5ab8-4dea-a436-719e84db873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1023\n",
      "Val dataset size: 114\n"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][idx]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][idx])\n",
    "        }\n",
    "\n",
    "train_dataset = QADataset(tokenized_train)\n",
    "val_dataset = QADataset(tokenized_val)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0b6f17-cce5-415a-ab58-76a82f2f2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./indobert-gizi-qa',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "    disable_tqdm=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18dc8cc6-c86d-41a5-80dd-bbb3fbfe3911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Memulai Fine-Tuning QA...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 07:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.108900</td>\n",
       "      <td>3.265762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.692400</td>\n",
       "      <td>3.037668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.217400</td>\n",
       "      <td>3.643459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.878300</td>\n",
       "      <td>3.207864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.352800</td>\n",
       "      <td>3.005722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.849300</td>\n",
       "      <td>3.371591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>3.491086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Menyimpan model...\n",
      "\n",
      "==================================================\n",
      "Fine-Tuning QA selesai!\n",
      "Model tersimpan di: ./indobert-gizi-qa-final\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Memulai Fine-Tuning QA...\")\n",
    "print(\"=\"*50)\n",
    "trainer.train()\n",
    "\n",
    "# Simpan model\n",
    "print(\"\\nüíæ Menyimpan model...\")\n",
    "model.save_pretrained('./indobert-gizi-qa-final')\n",
    "tokenizer.save_pretrained('./indobert-gizi-qa-final')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-Tuning QA selesai!\")\n",
    "print(\"Model tersimpan di: ./indobert-gizi-qa-final\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80a273-959f-48f5-9a43-8aa7378ed5a7",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5defa7ee-3e14-4d8f-9bae-201d962920e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ü§ñ HASIL TES MODEL QA\n",
      "==================================================\n",
      "Tanya : Apa itu diabetes melitus?\n",
      "Jawab : kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin\n",
      "Score : 0.0543\n",
      "------------------------------\n",
      "Tanya : Apa penyebab utama diabetes?\n",
      "Jawab : kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin\n",
      "Score : 0.0533\n",
      "------------------------------\n",
      "Tanya : Bagaimana cara mencegah ?\n",
      "Jawab : menjaga pola makan sehat dan rutin berolahraga.\n",
      "Score : 0.0310\n",
      "------------------------------\n",
      "Tanya : Apa gejala Diabetes melitus?\n",
      "Jawab : kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin\n",
      "Score : 0.0368\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, pipeline\n",
    "\n",
    "# 1. Load Model Final\n",
    "model_path = \"./indobert-gizi-qa-final\" # Pastikan path ini sesuai output training\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# 2. Buat Pipeline QA\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 # Gunakan GPU (0) atau CPU (-1)\n",
    ")\n",
    "\n",
    "# 3. Tes Manual\n",
    "context_text = \"\"\"\n",
    "Diabetes melitus adalah penyakit kronis yang ditandai dengan kadar gula darah yang tinggi. \n",
    "Penyebab utamanya adalah kurangnya produksi insulin oleh pankreas atau ketidakmampuan tubuh menggunakan insulin.\n",
    "Gejala umum meliputi sering haus, sering buang air kecil, dan penurunan berat badan drastis.\n",
    "Pencegahan dapat dilakukan dengan menjaga pola makan sehat dan rutin berolahraga.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Apa itu diabetes melitus?\",\n",
    "    \"Apa penyebab utama diabetes?\",\n",
    "    \"Bagaimana cara mencegah ?\",\n",
    "    \"Apa gejala Diabetes melitus?\"\n",
    "]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ü§ñ HASIL TES MODEL QA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for q in questions:\n",
    "    result = qa_pipeline(question=q, context=context_text)\n",
    "    print(f\"Tanya : {q}\")\n",
    "    print(f\"Jawab : {result['answer']}\")\n",
    "    print(f\"Score : {result['score']:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cf724-c243-4d16-be39-696e0128bb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce89d0e-45a4-45e3-a0d5-c376c9aa1e4b",
   "metadata": {},
   "source": [
    "# Tahap 5: Search FUnction (_or Search Engine, whaatever you called it_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dedd21d-d351-44b6-a19f-93aabaa10f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55abc1f-5d2e-4d18-ac9c-7beed708f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n",
      "Total chunks: 7661\n"
     ]
    }
   ],
   "source": [
    "# Load processed chunks\n",
    "print(\"üìÇ Loading dataset...\")\n",
    "df = pd.read_csv('output_dataset/processed_chunks.csv')\n",
    "print(f\"Total chunks: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0653ab-6f3b-4f89-8e77-04ffeabb47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading QA Model...\n",
      " QA Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load QA Model (Reader)\n",
    "print(\"\\n Loading QA Model...\")\n",
    "qa_model_path = './indobert-gizi-qa-final'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(qa_model_path)\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(qa_model_path)\n",
    "qa_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "qa_model.eval()\n",
    "print(\" QA Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef34c44b-f6ca-48a1-a8fa-9fd22c825d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RETRIEVER: Hybrid Search (Dense + BM25)\n",
    "# ============================================\n",
    "\n",
    "# Gunakan SentenceTransformer untuk embedding\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def custom_embedding_function(texts):\n",
    "    return embedding_model.encode(texts, show_progress_bar=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe97c643-6a77-4378-9e69-115300eb715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Setting up Retriever...\n",
      "  ‚Üí Initializing ChromaDB from: ./chroma_db_store\n"
     ]
    }
   ],
   "source": [
    "# Buat collection\n",
    "CHROMA_DATA_PATH = \"./chroma_db_store\" # Folder tempat data akan disimpan selamanya\n",
    "\n",
    "print(\"\\nüîç Setting up Retriever...\")\n",
    "\n",
    "# 1. Gunakan PERSISTENT Client (Bukan Client biasa)\n",
    "print(f\"  ‚Üí Initializing ChromaDB from: {CHROMA_DATA_PATH}\")\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "# Setup Embedding Function\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    ")\n",
    "\n",
    "# 2. Get or Create Collection\n",
    "# Fungsi ini akan meload collection jika ada, atau membuat baru jika belum ada\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"gizi_knowledge\",\n",
    "    embedding_function=embedding_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07076fab-1153-428b-8eea-0f61a252cad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚ö†Ô∏è Database kosong. Memulai proses Indexing Dokumen (Hanya sekali)...\n",
      "    Processed 500/7661 chunks\n",
      "    Processed 1000/7661 chunks\n",
      "    Processed 1500/7661 chunks\n",
      "    Processed 2000/7661 chunks\n",
      "    Processed 2500/7661 chunks\n",
      "    Processed 3000/7661 chunks\n",
      "    Processed 3500/7661 chunks\n",
      "    Processed 4000/7661 chunks\n",
      "    Processed 4500/7661 chunks\n",
      "    Processed 5000/7661 chunks\n",
      "    Processed 5500/7661 chunks\n",
      "    Processed 6000/7661 chunks\n",
      "    Processed 6500/7661 chunks\n",
      "    Processed 7000/7661 chunks\n",
      "    Processed 7500/7661 chunks\n",
      "  ‚úÖ Indexing Selesai & Tersimpan Otomatis!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOGIKA PENGISIAN DATA CERDAS\n",
    "# ============================================\n",
    "\n",
    "# Cek apakah database sudah berisi data?\n",
    "if collection.count() == 0:\n",
    "    print(\"  ‚ö†Ô∏è Database kosong. Memulai proses Indexing Dokumen (Hanya sekali)...\")\n",
    "    \n",
    "    # --- PROSES ADD DOCUMENT (Sama seperti kode lama Anda) ---\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        collection.add(\n",
    "            documents=batch['text'].tolist(),\n",
    "            ids=[f\"chunk_{j}\" for j in range(i, min(i+batch_size, len(df)))],\n",
    "            metadatas=[\n",
    "                {\n",
    "                    'title': str(row['title']),\n",
    "                    'url': str(row['url']),\n",
    "                    'intent': str(row['intent']),\n",
    "                    'doc_id': int(row['doc_id']),\n",
    "                    'chunk_id': int(row['chunk_id'])\n",
    "                }\n",
    "                for _, row in batch.iterrows()\n",
    "            ]\n",
    "        )\n",
    "        if (i + batch_size) % 500 == 0:\n",
    "            print(f\"    Processed {min(i+batch_size, len(df))}/{len(df)} chunks\")\n",
    "            \n",
    "    print(\"  ‚úÖ Indexing Selesai & Tersimpan Otomatis!\")\n",
    "\n",
    "else:\n",
    "    print(f\"  ‚úÖ Database ditemukan berisi {collection.count()} dokumen.\")\n",
    "    print(\"  ‚è© Skip indexing. Langsung siap dipakai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92af591d-34ed-400a-a91e-6e092e2ce575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí Initializing BM25...\n",
      "‚úÖ BM25 ready\n"
     ]
    }
   ],
   "source": [
    "# 2. BM25 (Sparse Retrieval)\n",
    "print(\"  ‚Üí Initializing BM25...\")\n",
    "tokenized_corpus = [doc.lower().split() for doc in df['text'].tolist()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(\"‚úÖ BM25 ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c699db97-57e1-42e7-9c41-474191b1016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENSEMBLE RETRIEVER\n",
    "# ============================================\n",
    "\n",
    "def hybrid_retrieve(query, top_k=5, dense_weight=0.6, bm25_weight=0.4):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval: Dense (ChromaDB) + Sparse (BM25)\n",
    "    \"\"\"\n",
    "    # Dense retrieval\n",
    "    dense_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k * 2\n",
    "    )\n",
    "    \n",
    "    dense_docs = []\n",
    "    for i, doc_id in enumerate(dense_results['ids'][0]):\n",
    "        idx = int(doc_id.split('_')[1])\n",
    "        dense_docs.append({\n",
    "            'idx': idx,\n",
    "            'text': dense_results['documents'][0][i],\n",
    "            'metadata': dense_results['metadatas'][0][i],\n",
    "            'distance': dense_results['distances'][0][i],\n",
    "            'dense_score': 1 / (1 + dense_results['distances'][0][i])  # Convert distance to score\n",
    "        })\n",
    "    \n",
    "    # BM25 retrieval\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[::-1][:top_k * 2]\n",
    "    \n",
    "    bm25_docs = []\n",
    "    for idx in top_bm25_indices:\n",
    "        bm25_docs.append({\n",
    "            'idx': int(idx),\n",
    "            'bm25_score': float(bm25_scores[idx])\n",
    "        })\n",
    "    \n",
    "    # Combine scores\n",
    "    combined = {}\n",
    "    for doc in dense_docs:\n",
    "        idx = doc['idx']\n",
    "        combined[idx] = {\n",
    "            'text': doc['text'],\n",
    "            'metadata': doc['metadata'],\n",
    "            'score': dense_weight * doc['dense_score']\n",
    "        }\n",
    "    \n",
    "    for doc in bm25_docs:\n",
    "        idx = doc['idx']\n",
    "        if idx in combined:\n",
    "            combined[idx]['score'] += bm25_weight * doc['bm25_score']\n",
    "        else:\n",
    "            combined[idx] = {\n",
    "                'text': df.iloc[idx]['text'],\n",
    "                'metadata': {\n",
    "                    'title': df.iloc[idx]['title'],\n",
    "                    'url': df.iloc[idx]['url'],\n",
    "                    'intent': df.iloc[idx]['intent']\n",
    "                },\n",
    "                'score': bm25_weight * doc['bm25_score']\n",
    "            }\n",
    "    \n",
    "    # Sort by combined score\n",
    "    sorted_results = sorted(combined.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            'text': item[1]['text'],\n",
    "            'metadata': item[1]['metadata'],\n",
    "            'score': item[1]['score']\n",
    "        }\n",
    "        for item in sorted_results[:top_k]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76fc958b-313f-4a92-b4f5-ff983bef1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, confidence_threshold=0.0): # Set 0.0 biar kita lihat SEMUA hasil\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    ).to(qa_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = qa_model(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits[0]\n",
    "    end_logits = outputs.end_logits[0]\n",
    "    \n",
    "    # Ambil posisi terbaik\n",
    "    start_idx = torch.argmax(start_logits).item()\n",
    "    end_idx = torch.argmax(end_logits).item()\n",
    "    \n",
    "    # Hitung probabilitas\n",
    "    start_probs = torch.softmax(start_logits, dim=0)\n",
    "    end_probs = torch.softmax(end_logits, dim=0)\n",
    "    confidence = (start_probs[start_idx] * end_probs[end_idx]).item()\n",
    "    \n",
    "    # --- BAGIAN DEBUGGING ---\n",
    "    print(f\"\\n   üïµÔ∏è [DEBUG] Q: {question}\")\n",
    "    print(f\"   üìç Start Index: {start_idx} | End Index: {end_idx}\")\n",
    "    print(f\"   üìä Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Cek apakah dia menunjuk [CLS] (Index 0)\n",
    "    if start_idx == 0:\n",
    "        print(\"   ‚ùå Model memprediksi: [CLS] (Unanswerable / Menyerah)\")\n",
    "        return None, confidence\n",
    "\n",
    "    # Cek apakah start > end (Mustahil)\n",
    "    if start_idx > end_idx:\n",
    "        print(\"   ‚ùå Model memprediksi: Posisi Terbalik (Start > End)\")\n",
    "        return None, confidence\n",
    "\n",
    "    # Extract jawaban\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    answer_tokens = tokens[start_idx:end_idx + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens).replace('##', '')\n",
    "    \n",
    "    print(f\"   ‚úÖ Calon Jawaban: {answer}\")\n",
    "    return answer, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56656a97-2b0a-4f2a-add2-194d96c7b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHATBOT PIPELINE (MODE BYPASS / SEARCH ENGINE)\n",
    "# ============================================\n",
    "\n",
    "def chatbot_pipeline(question, top_k=3):\n",
    "    print(f\"\\n Question: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Cari Dokumen (Retriever tetap bekerja!)\n",
    "    print(\" Retrieving relevant documents...\")\n",
    "    retrieved_docs = hybrid_retrieve(question, top_k=top_k)\n",
    "    \n",
    "    # Fallback jika tidak ada dokumen\n",
    "    if not retrieved_docs:\n",
    "        return {\n",
    "            'answer': 'Maaf, saya belum menemukan informasi yang relevan di database.',\n",
    "            'source': None,\n",
    "            'confidence': 0.0,\n",
    "            'intent': 'Unknown'\n",
    "        }\n",
    "        \n",
    "    print(f\"‚úÖ Found {len(retrieved_docs)} relevant documents\")\n",
    "    \n",
    "    # 2. BYPASS QA MODEL\n",
    "    # Alih-alih menyuruh model QA mikir, kita langsung ambil dokumen terbaik (Top 1)\n",
    "    best_doc = retrieved_docs[0] \n",
    "    \n",
    "    # Ambil snippet (misal 3 kalimat pertama atau 300 karakter)\n",
    "    full_text = best_doc['text']\n",
    "    # Potong biar gak kepanjangan\n",
    "    snippet = full_text[:350] + \"...\" if len(full_text) > 350 else full_text\n",
    "    \n",
    "    # Format jawaban ala Search Engine\n",
    "    response = (\n",
    "        f\"Berikut informasi yang saya temukan:\\n\\n\"\n",
    "        f\"\\\"{snippet}\\\"\\n\\n\"\n",
    "        f\"(Sumber: {best_doc['metadata']['title']})\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response,\n",
    "        'source': best_doc['metadata']['url'],\n",
    "        'confidence': best_doc['score'], # Pakai skor relevansi pencarian\n",
    "        'intent': best_doc['metadata']['intent']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "787ba9b5-ed38-4a95-b110-2209aa872e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing pipeline...\n",
      "\n",
      "‚ùì Question: Apa penyebab perut buncit?\n",
      "============================================================\n",
      "üîç Retrieving relevant documents...\n",
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      " Answer: Berikut informasi yang saya temukan:\n",
      "\n",
      "\"10 makanan penyebab perut buncit no . 7 sering dikonsumsi . makanan penyebab perut buncit bisa menjadi penyebab penumpukan lemak di perut . kondisi ini bisa meningkatkan berbagai risiko penyakit , namun masih bisa diatasi dengan asupan nutrisi yang tepat , meningkatkan aktivitas fisik , hingga mengurangi stres . lalu , apa saja makanan yang harus d...\"\n",
      "\n",
      "(Sumber: 10 Makanan Penyebab Perut Buncit (No. 7 Sering Dikonsumsi))\n",
      " Source: https://doktersehat.com/gaya-hidup/gizi-dan-nutrisi/makanan-penyebab-perut-buncit/\n",
      " Confidence: 399.97%\n",
      "  Intent: berat_badan, Kesehatan_umum, kesehatan_ibu\n",
      "\n",
      "‚ùì Question: Bagaimana cara menurunkan berat badan?\n",
      "============================================================\n",
      "üîç Retrieving relevant documents...\n",
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      " Answer: Berikut informasi yang saya temukan:\n",
      "\n",
      "\"alasan ketidaksuburan pada perempuan . selain itu , beberapa tikus betina lainnya juga menunjukkan menstruasi yang tidak rutin dan gangguan reproduksi lainnya . penelitian lain menyebutkan bahwa polycystic ovary syndrome mengakibatkan peningkatan hormon androgen hormon laki - laki pada perempuan , dan mengganggu ovarium untuk menghasilkan telur . b...\"\n",
      "\n",
      "(Sumber: Obesitas Mengurangi Kesuburan Wanita)\n",
      " Source: https://hellosehat.com/nutrisi/obesitas/kelebihan-berat-badan-mengurangi-kesuburan-wanita/\n",
      " Confidence: 284.33%\n",
      "  Intent: berat_badan, kesehatan_ibu, diabetes\n",
      "\n",
      "‚ùì Question: Makanan apa yang baik untuk diet?\n",
      "============================================================\n",
      "üîç Retrieving relevant documents...\n",
      "‚úÖ Found 3 relevant documents\n",
      "\n",
      " Answer: Berikut informasi yang saya temukan:\n",
      "\n",
      "\"8 makanan setelah olahraga agar kembali berstamina . makan makanan setelah olahraga memang sangat diperlukan . pasalnya , aktivitas olahraga sangat melelahkan . sehingga , dibutuhkan asupan makanan dan minuman yang membantu menggantikan energi yang hilang . simak makanan apa saja yang disarankan untuk anda konsumsi sehabis berolahraga berikut ini !...\"\n",
      "\n",
      "(Sumber: 8 Makanan Setelah Olahraga agar Kembali Berstamina)\n",
      " Source: https://doktersehat.com/gaya-hidup/gizi-dan-nutrisi/makanan-setelah-olahraga/\n",
      " Confidence: 585.41%\n",
      "  Intent: fakta_gizi, pembentukan_tubuh, makanan_sehat\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TESTING\n",
    "# ============================================\n",
    "# Test queries\n",
    "test_questions = [\n",
    "    \"Apa penyebab perut buncit?\",\n",
    "    \"Bagaimana cara menurunkan berat badan?\",\n",
    "    \"Makanan apa yang baik untuk diet?\"\n",
    "]\n",
    "\n",
    "print(\"\\n Testing pipeline...\")\n",
    "for q in test_questions:\n",
    "    result = chatbot_pipeline(q)\n",
    "    print(f\"\\n Answer: {result['answer']}\")\n",
    "    print(f\" Source: {result['source']}\")\n",
    "    print(f\" Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"  Intent: {result['intent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775551cf-a2b1-434b-bd39-b02db7bc3923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
