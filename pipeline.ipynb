{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5670a488-7d78-4236-809a-17c42b29ecff",
   "metadata": {},
   "source": [
    "# Import and combine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e01e31e-4096-4278-afd3-5f0c2b5aa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('original_dataset/hellosehat_dataset_10.csv', sep=';', encoding='utf-8')\n",
    "df2 = pd.read_csv('original_dataset/alodokter_dataset_10.csv', sep=';', encoding='utf-8')\n",
    "df3 = pd.read_csv('original_dataset/doktersehat_gizi_final_10.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45fe201-1e73-40a7-a122-178c1278ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dokumen setelah concat: 2391\n",
      "Index range: 0 - 2390\n",
      "Kolom: ['URL', 'Judul', 'Konten']\n"
     ]
    }
   ],
   "source": [
    "df_combine = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(f\"Total dokumen setelah concat: {len(df_combine)}\")\n",
    "print(f\"Index range: {df_combine.index.min()} - {df_combine.index.max()}\")\n",
    "print(f\"Kolom: {df_combine.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0983142-15de-423f-bfe9-856d8eec8d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Judul</th>\n",
       "      <th>Konten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Kenali 9 Penyebab Perut Buncit dan Cara Mengat...</td>\n",
       "      <td>Perut buncit memang mampu memengaruhi penampil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hellosehat.com/nutrisi/tips-makan-seha...</td>\n",
       "      <td>8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue</td>\n",
       "      <td>Bagi Anda yang gemar bikin kue, oven gas menja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hellosehat.com/nutrisi/fakta-gizi/mere...</td>\n",
       "      <td>10 Merek Oatmeal yang Bergizi dan Cocok untuk ...</td>\n",
       "      <td>Butuh menu sarapan yang cepat? Berbagai merek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hellosehat.com/nutrisi/resep-sehat/jus...</td>\n",
       "      <td>4 Resep Jus untuk Bantu Meningkatkan Sistem Im...</td>\n",
       "      <td>Setiap harinya, sistem imunitas pada tubuh bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Apakah Sering Buang Air Bisa Menurunkan Berat ...</td>\n",
       "      <td>Setelah diolah, dicerna, dan diambil semua giz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "1  https://hellosehat.com/nutrisi/tips-makan-seha...   \n",
       "2  https://hellosehat.com/nutrisi/fakta-gizi/mere...   \n",
       "3  https://hellosehat.com/nutrisi/resep-sehat/jus...   \n",
       "4  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "\n",
       "                                               Judul  \\\n",
       "0  Kenali 9 Penyebab Perut Buncit dan Cara Mengat...   \n",
       "1    8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue   \n",
       "2  10 Merek Oatmeal yang Bergizi dan Cocok untuk ...   \n",
       "3  4 Resep Jus untuk Bantu Meningkatkan Sistem Im...   \n",
       "4  Apakah Sering Buang Air Bisa Menurunkan Berat ...   \n",
       "\n",
       "                                              Konten  \n",
       "0  Perut buncit memang mampu memengaruhi penampil...  \n",
       "1  Bagi Anda yang gemar bikin kue, oven gas menja...  \n",
       "2  Butuh menu sarapan yang cepat? Berbagai merek ...  \n",
       "3  Setiap harinya, sistem imunitas pada tubuh bek...  \n",
       "4  Setelah diolah, dicerna, dan diambil semua giz...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d15e7e-91e5-4582-b503-b4e902ad18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX that thang!\n",
    "# df_combine.to_csv('output_dataset/combined_nutrition_dataset_2.csv', index=False, encoding='utf-8', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240d6b6f-ee09-46f7-8ef3-12d7e0e96db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Judul</th>\n",
       "      <th>Konten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Kenali 9 Penyebab Perut Buncit dan Cara Mengat...</td>\n",
       "      <td>Perut buncit memang mampu memengaruhi penampil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hellosehat.com/nutrisi/tips-makan-seha...</td>\n",
       "      <td>8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue</td>\n",
       "      <td>Bagi Anda yang gemar bikin kue, oven gas menja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hellosehat.com/nutrisi/fakta-gizi/mere...</td>\n",
       "      <td>10 Merek Oatmeal yang Bergizi dan Cocok untuk ...</td>\n",
       "      <td>Butuh menu sarapan yang cepat? Berbagai merek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hellosehat.com/nutrisi/resep-sehat/jus...</td>\n",
       "      <td>4 Resep Jus untuk Bantu Meningkatkan Sistem Im...</td>\n",
       "      <td>Setiap harinya, sistem imunitas pada tubuh bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hellosehat.com/nutrisi/berat-badan-tur...</td>\n",
       "      <td>Apakah Sering Buang Air Bisa Menurunkan Berat ...</td>\n",
       "      <td>Setelah diolah, dicerna, dan diambil semua giz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "1  https://hellosehat.com/nutrisi/tips-makan-seha...   \n",
       "2  https://hellosehat.com/nutrisi/fakta-gizi/mere...   \n",
       "3  https://hellosehat.com/nutrisi/resep-sehat/jus...   \n",
       "4  https://hellosehat.com/nutrisi/berat-badan-tur...   \n",
       "\n",
       "                                               Judul  \\\n",
       "0  Kenali 9 Penyebab Perut Buncit dan Cara Mengat...   \n",
       "1    8 Merk Oven Gas Terbaik, Cocok untuk Bisnis Kue   \n",
       "2  10 Merek Oatmeal yang Bergizi dan Cocok untuk ...   \n",
       "3  4 Resep Jus untuk Bantu Meningkatkan Sistem Im...   \n",
       "4  Apakah Sering Buang Air Bisa Menurunkan Berat ...   \n",
       "\n",
       "                                              Konten  \n",
       "0  Perut buncit memang mampu memengaruhi penampil...  \n",
       "1  Bagi Anda yang gemar bikin kue, oven gas menja...  \n",
       "2  Butuh menu sarapan yang cepat? Berbagai merek ...  \n",
       "3  Setiap harinya, sistem imunitas pada tubuh bek...  \n",
       "4  Setelah diolah, dicerna, dan diambil semua giz...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('output_dataset/combined_nutrition_dataset.csv', sep=';', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad3623e6-a206-41f4-855c-2f3735e0fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL       https://doktersehat.com/gaya-hidup/gizi-dan-nu...\n",
      "Judul        Sumber, Manfaat, dan Dampak Kekurangan Omega-3\n",
      "Konten     Kita selalu menganggap kalau omega-3 adalah n...\n",
      "Name: 2150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "row = df.loc[2150]\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef974a-662b-448a-aea8-f8f33c409842",
   "metadata": {},
   "source": [
    "# Intent Tagging (Health Goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddde1b5b-86f7-4c8d-9797-fb69ff3c761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1008306-2c1f-46f1-a3b8-1140ec3bd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. KONFIGURASI INPUT & OUTPUT ---\n",
    "INPUT_FILE = 'output_dataset/combined_nutrition_dataset.csv'\n",
    "OUTPUT_FILE = 'output_dataset/tagged_combined_nutrition_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7d2538-55a2-4ad3-9190-b7ec15f0b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. KAMUS KEYWORD INTENT ---\n",
    "INTENT_KEYWORDS = {\n",
    "    'diabetes': ['diabetes', 'gula darah', 'kencing manis', 'insulin', 'glukosa', 'hiperglikemia'],\n",
    "    'anemia': ['anemia', 'kurang darah', 'zat besi', 'hemoglobin', 'pucat', 'lelah'],\n",
    "    'kesehatan_ibu': ['hamil', 'menyusui', 'bumil', 'busui', 'asi', 'janin', 'kandungan', 'kehamilan'],\n",
    "    'kesehatan_anak': ['anak', 'bayi', 'balita', 'si kecil', 'tumbuh kembang', 'imunisasi'],\n",
    "    'berat_badan': ['berat badan', 'diet', 'kurus', 'gemuk', 'langsing', 'turun berat', 'buncit', 'lemak', 'kalori', 'obesitas'],\n",
    "    'pembentukan_tubuh': ['otot', 'gym', 'fitness', 'binaraga', 'sixpack', 'latihan beban', 'workout', 'massa otot'],\n",
    "    'kesehatan_pencernaan': ['pencernaan', 'usus', 'lambung', 'maag', 'gerd', 'sembelit', 'diare', 'serat'],\n",
    "    'resep_sehat': ['resep', 'cara membuat', 'bahan-bahan', 'menu masakan', 'cara masak', 'hidangan'],\n",
    "    'diet_khusus': ['keto', 'vegan', 'vegetarian', 'gluten free', 'rendah garam', 'dash diet', 'intermittent'],\n",
    "    'pencegahan': ['mencegah', 'risiko', 'hindari', 'bahaya', 'waspada', 'gejala', 'tanda-tanda'],\n",
    "    'fakta_gizi': ['kandungan gizi', 'nutrisi', 'protein', 'karbohidrat', 'vitamin', 'mineral', 'takaran saji'],\n",
    "    'makanan_sehat': ['buah', 'sayur', 'organik', 'superfood', 'makanan sehat', 'bijian', 'kacang'],\n",
    "    # Fallback\n",
    "    'Kesehatan_umum': ['manfaat', 'khasiat', 'sehat', 'bugar', 'stamina', 'daya tahan', 'imun', 'kesehatan']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c96a7416-7d41-4040-aa40-11eebc72edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_3_intents(text):\n",
    "    \"\"\"\n",
    "    Menghitung frekuensi keyword dan mengembalikan maksimal 3 intent teratas.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for intent, keywords in INTENT_KEYWORDS.items():\n",
    "        count = 0\n",
    "        for kw in keywords:\n",
    "            count += text.count(kw)\n",
    "        if count > 0:\n",
    "            scores[intent] = count\n",
    "    \n",
    "    # Jika tidak ada match, return default\n",
    "    if not scores:\n",
    "        return \"Kesehatan_umum\"\n",
    "    \n",
    "    # Urutkan score tertinggi -> terendah\n",
    "    sorted_intents = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Ambil 3 teratas\n",
    "    top_3 = [item[0] for item in sorted_intents[:3]]\n",
    "    return \", \".join(top_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "288ce2ad-f4fa-4ee9-af29-702e23b8ef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Membaca file input: output_dataset/combined_nutrition_dataset.csv ...\n",
      "üìä Total Data: 2425 baris.\n",
      "üîç Melakukan Auto-Tagging Intent...\n",
      "üíæ Menyimpan ke format final (Comma Separated + Quoted)...\n",
      "‚úÖ SUKSES! File tersimpan: output_dataset/tagged_combined_nutrition_dataset.csv\n",
      "   Format: \"URL\",\"Title\",\"Content\",\"Intent\" (Comma Separated)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. EKSEKUSI UTAMA ---\n",
    "def process_tagging():\n",
    "    print(f\"üìÇ Membaca file input: {INPUT_FILE} ...\")\n",
    "    \n",
    "    try:\n",
    "        # Membaca file dengan separator ';'\n",
    "        # Kita asumsikan baris pertama adalah header [URL;Judul;Konten]\n",
    "        df = pd.read_csv(INPUT_FILE, sep=';', on_bad_lines='skip')\n",
    "        \n",
    "        # Validasi kolom dasar (Sesuaikan nama kolom jika di file Anda berbeda)\n",
    "        # Kita rename standar agar mudah diproses\n",
    "        # Asumsi urutan kolom: 1. URL, 2. Judul, 3. Konten\n",
    "        if len(df.columns) >= 3:\n",
    "            df.columns = ['url', 'title', 'content'] + list(df.columns[3:])\n",
    "        else:\n",
    "            print(\"‚ùå Error: File input harus memiliki minimal 3 kolom (URL;Judul;Konten)\")\n",
    "            return\n",
    "\n",
    "        print(f\"üìä Total Data: {len(df)} baris.\")\n",
    "        \n",
    "        # Bersihkan data (isi yang kosong dengan string kosong)\n",
    "        df['title'] = df['title'].fillna('').astype(str)\n",
    "        df['content'] = df['content'].fillna('').astype(str)\n",
    "        \n",
    "        print(\"üîç Melakukan Auto-Tagging Intent...\")\n",
    "        # Gabungkan Judul + Konten untuk pencarian keyword yang lebih akurat\n",
    "        df['full_text_scan'] = df['title'] + \" \" + df['content']\n",
    "        df['intent'] = df['full_text_scan'].apply(get_top_3_intents)\n",
    "        \n",
    "        # Hapus kolom bantuan\n",
    "        df.drop(columns=['full_text_scan'], inplace=True)\n",
    "        \n",
    "        print(f\"üíæ Menyimpan ke format final (Comma Separated + Quoted)...\")\n",
    "        \n",
    "        # --- TEKNIK PENYIMPANAN PENTING ---\n",
    "        # quoting=csv.QUOTE_ALL : Memaksa SEMUA kolom dibungkus tanda kutip \"...\"\n",
    "        # Ini menjamin koma di dalam teks TIDAK akan dianggap sebagai pemisah kolom baru.\n",
    "        df.to_csv(\n",
    "            OUTPUT_FILE, \n",
    "            sep=',',              # Separator Koma\n",
    "            quotechar='\"',        # Pembungkus Tanda Kutip Ganda\n",
    "            quoting=csv.QUOTE_ALL, # MODE AMAN: Bungkus semua data dengan kutip\n",
    "            index=False,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ SUKSES! File tersimpan: {OUTPUT_FILE}\")\n",
    "        print(\"   Format: \\\"URL\\\",\\\"Title\\\",\\\"Content\\\",\\\"Intent\\\" (Comma Separated)\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {INPUT_FILE} tidak ditemukan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Terjadi kesalahan: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2710d3-660b-44e8-a980-bb0db5dbbe54",
   "metadata": {},
   "source": [
    "# Preprocessing and chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51677a25-e972-4303-8bc2-423413df53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e1a411-75b5-49bd-883f-ec83ea4b04f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dokumen: 2390\n",
      "Kolom dataset: ['URL', 'Judul', 'Konten']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('tagged_combined_nutrition_dataset.csv')\n",
    "print(f\"Total dokumen: {len(df)}\")\n",
    "print(f\"Kolom dataset: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5140e8fc-504b-49f8-894d-3829b083b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IndoBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Normalisasi teks\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s.,!?%-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Fungsi chunking dengan sliding window\n",
    "def chunk_text_with_overlap(text, max_tokens=384, overlap=50):\n",
    "    \"\"\"\n",
    "    Chunking dengan overlap sliding window\n",
    "    max_tokens: 512 - 128 (buffer untuk question) = 384\n",
    "    overlap: 50 tokens\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a478aa-4ed5-4d65-b52a-882228adaf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/2390 documents\n",
      "Processed 200/2390 documents\n",
      "Processed 300/2390 documents\n",
      "Processed 400/2390 documents\n",
      "Processed 500/2390 documents\n",
      "Processed 600/2390 documents\n",
      "Processed 700/2390 documents\n",
      "Processed 800/2390 documents\n",
      "Processed 900/2390 documents\n",
      "Processed 1000/2390 documents\n",
      "Processed 1100/2390 documents\n",
      "Processed 1200/2390 documents\n",
      "Processed 1300/2390 documents\n",
      "Processed 1400/2390 documents\n",
      "Processed 1500/2390 documents\n",
      "Processed 1600/2390 documents\n",
      "Processed 1700/2390 documents\n",
      "Processed 1800/2390 documents\n",
      "Processed 1900/2390 documents\n",
      "Processed 2000/2390 documents\n",
      "Processed 2100/2390 documents\n",
      "Processed 2200/2390 documents\n",
      "Processed 2300/2390 documents\n",
      "\n",
      "Total chunks: 7661\n",
      "Rata-rata tokens per chunk: 334.99\n",
      "Max tokens: 388\n",
      "Min tokens: 33\n"
     ]
    }
   ],
   "source": [
    "# Proses semua dokumen\n",
    "processed_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    title = row.get('Judul', '') if 'Judul' in df.columns else ''\n",
    "    content = row.get('Konten', '') if 'Konten' in df.columns else row.get('text', '')\n",
    "    url = row.get('URL', '') if 'URL' in df.columns else ''\n",
    "    \n",
    "    full_text = f\"{title}. {content}\".strip()\n",
    "    cleaned_text = preprocess_text(full_text)\n",
    "    \n",
    "    if not cleaned_text:\n",
    "        continue\n",
    "    \n",
    "    chunks = chunk_text_with_overlap(cleaned_text)\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        processed_data.append({\n",
    "            'doc_id': idx,\n",
    "            'chunk_id': chunk_idx,\n",
    "            'title': title,\n",
    "            'text': chunk,\n",
    "            'url': url,\n",
    "            'token_count': len(tokenizer.tokenize(chunk))\n",
    "        })\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} documents\")\n",
    "\n",
    "# Simpan hasil\n",
    "df_processed = pd.DataFrame(processed_data)\n",
    "df_processed.to_csv('processed_chunks.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(df_processed)}\")\n",
    "print(f\"Rata-rata tokens per chunk: {df_processed['token_count'].mean():.2f}\")\n",
    "print(f\"Max tokens: {df_processed['token_count'].max()}\")\n",
    "print(f\"Min tokens: {df_processed['token_count'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e632e6a-5868-4631-a02e-e82c4913b7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File tersimpan:\n",
      "- processed_chunks.csv\n",
      "- corpus_for_mlm.json\n"
     ]
    }
   ],
   "source": [
    "# Simpan corpus untuk MLM\n",
    "all_texts = df_processed['text'].tolist()\n",
    "with open('corpus_for_mlm.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_texts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nFile tersimpan:\")\n",
    "print(\"- processed_chunks.csv\")\n",
    "print(\"- corpus_for_mlm.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6a194-3675-4c7f-b714-77518f400d94",
   "metadata": {},
   "source": [
    "# Domain ~~EXPANSION~~ Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1e876d-3d87-4431-ab70-8618c96d31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 07:01:24.452778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForMaskedLM,\n",
    "    DataCollatorForWholeWordMask,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a13760-3d4b-4012-839c-e57e6f5c83dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts: 7661\n"
     ]
    }
   ],
   "source": [
    "# Load corpus\n",
    "with open('corpus_for_mlm.json', 'r', encoding='utf-8') as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "print(f\"Total texts: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4626f3-3878-4529-ad12-46e3597822b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model dan tokenizer\n",
    "model_name = 'indobenchmark/indobert-base-p2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Custom Dataset untuk MLM\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Preprocess dengan progress bar\n",
    "        print(\"Preprocessing texts...\")\n",
    "        self.encodings = []\n",
    "        for text in tqdm(texts, desc=\"Tokenizing\"):\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.encodings.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze()\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a155e71c-dd08-4b4d-9a4d-eb2476632623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Data Collator untuk WWM tanpa masking angka & satuan\n",
    "class CustomWWMDataCollator(DataCollatorForWholeWordMask):\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15):\n",
    "        super().__init__(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=True,\n",
    "            mlm_probability=mlm_probability\n",
    "        )\n",
    "    \n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask=None):\n",
    "        \"\"\"Override untuk hindari masking angka & satuan\"\"\"\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        \n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "                for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "        \n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        \n",
    "        # Hindari masking angka & satuan\n",
    "        for i, input_ids in enumerate(inputs):\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            for j, token in enumerate(tokens):\n",
    "                # Skip jika token mengandung angka atau satuan umum\n",
    "                if re.search(r'\\d', token) or token in ['mg', 'gram', 'kg', 'ml', 'kkal', 'kalori', '%']:\n",
    "                    probability_matrix[i, j] = 0.0\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100\n",
    "        \n",
    "        # 80% replaced with [MASK]\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        \n",
    "        # 10% replaced with random token\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "        \n",
    "        # 10% unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f05a606-26a7-467a-91f9-84eedfee144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mempersiapkan dataset...\n",
      "Preprocessing texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283605a30d274e2b92ed07d6533a4739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/7661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhalo/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/data/data_collator.py:1642: FutureWarning: DataCollatorForWholeWordMask is deprecated and will be removed in a future version, you can now use DataCollatorForLanguageModeling with whole_word_mask=True instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Buat dataset\n",
    "print(\"\\nMempersiapkan dataset...\")\n",
    "train_dataset = MLMDataset(texts, tokenizer)\n",
    "\n",
    "# Data collator dengan WWM\n",
    "data_collator = CustomWWMDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./indobert-gizi-mlm',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    disable_tqdm=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbd5d79-d6b7-416b-8c13-675e900ef7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Memulai Domain Adaptation...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1916' max='1916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1916/1916 3:38:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.746800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.318100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan model...\n",
      "\n",
      "==================================================\n",
      "Domain Adaptation selesai!\n",
      "Model tersimpan di: ./indobert-gizi-mlm-final\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Memulai Domain Adaptation...\")\n",
    "print(\"=\"*50)\n",
    "trainer.train()\n",
    "\n",
    "# Simpan model\n",
    "print(\"\\nMenyimpan model...\")\n",
    "model.save_pretrained('./indobert-gizi-mlm-final')\n",
    "tokenizer.save_pretrained('./indobert-gizi-mlm-final')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Domain Adaptation selesai!\")\n",
    "print(\"Model tersimpan di: ./indobert-gizi-mlm-final\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
